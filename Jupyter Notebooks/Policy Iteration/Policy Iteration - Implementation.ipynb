{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083511cf",
   "metadata": {},
   "source": [
    "## Problem 1: Sample Recolection\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step to applying the Policy Iteration Algorithm to this problem will be coding all of the initial conditions of the problem. We start by coding the time steps, the possible number of samples and the possible actions to be taken. Due to the fact that this is a finite, *non-stationary* MDP, combinations of time steps and number of samples will be considered the states of the problem. For example, 1250 samples at 500m of immersion will be an state and 1250 samples at 400m of immersion will be another state. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927f71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time steps of decision\n",
    "T = [1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0]\n",
    "\n",
    "### Samples\n",
    "Samples = [0, 250, 500, 750, 1000, 1250, \"Succes\"]\n",
    "\n",
    "### Space of States\n",
    "S_t = []\n",
    "for S in [0, 250, 500, 750]:\n",
    "    S_t.append((1000,S))\n",
    "for T in [900, 800, 700, 600, 500, 400, 300, 200, 100, 0]:\n",
    "    for S in Samples:\n",
    "        S_t.append((T,S))\n",
    "\n",
    "### Actions \n",
    "# CM for Calculated Maneuver\n",
    "# IM for improvised Maneuver \n",
    "A = [\"CM\", \"IM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecd067",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Next, transition probabilities will be coded. As mention before, this probabilities will indicate the stochastic nature of the problem on the transitions between states. As the states contain steps of decision and number of samples, this information will be taken into account in the transition probabilities. Also, the probabilities depend on the action that is performed. Therefore, there will be to dictionaries of probabilites, one for Calculated Maneuvers and one for Improvised Maneuvers. At the end, both of this dictionaries will be consolidated in one dictionary. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a30974",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transition probabilities\n",
    "## Transition Probabilites will be defined as dictionaries\n",
    "# Transition Probabilities for Calculated Maneuvers\n",
    "p_CM = {}\n",
    "for s_t in S_t:\n",
    "    for s_tplus1 in S_t:\n",
    "        if s_tplus1[0] == s_t[0] - 100:\n",
    "            if s_t[1] != \"Succes\" and s_tplus1[1] != \"Succes\":\n",
    "                if s_tplus1[1] == s_t[1] + 250 and s_t[1] < 1250:\n",
    "                    p_CM[s_t, s_tplus1] = 0.3\n",
    "                elif s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:\n",
    "                    p_CM[s_t, s_tplus1] = 0.2\n",
    "                elif s_tplus1[1] == s_t[1] + 750 and s_t[1] < 750:\n",
    "                    p_CM[s_t, s_tplus1] = 0.1\n",
    "                elif s_tplus1[1] == s_t[1] and s_t[1] != \"Succes\":\n",
    "                    p_CM[s_t, s_tplus1] = 0.4\n",
    "                else: \n",
    "                    p_CM[s_t, s_tplus1] = 0\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 1000:\n",
    "                p_CM[s_t, s_tplus1] = 0.3\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 1250:\n",
    "                p_CM[s_t, s_tplus1] = 0.6\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 750:\n",
    "                p_CM[s_t, s_tplus1] = 0.1\n",
    "            elif s_tplus1[1] == s_t[1] and s_t[1] == \"Succes\":\n",
    "                p_CM[s_t, s_tplus1] = 1\n",
    "            else: \n",
    "                p_CM[s_t, s_tplus1] = 0\n",
    "        else:\n",
    "            p_CM[s_t, s_tplus1] = 0\n",
    "    \n",
    "# Transition Probabilites for Improvised Maneuvers\n",
    "p_IM = {}\n",
    "for s_t in S_t:\n",
    "    for s_tplus1 in S_t:\n",
    "        if s_tplus1[0] == s_t[0] - 100:\n",
    "            if s_t[1] != \"Succes\" and s_tplus1[1] != \"Succes\":\n",
    "                if s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:\n",
    "                    p_IM[s_t, s_tplus1] = 0.5\n",
    "                elif s_tplus1[1] == s_t[1] and s_t[1] <= 1250:\n",
    "                    p_IM[s_t, s_tplus1] = 0.5\n",
    "                else: \n",
    "                    p_IM[s_t, s_tplus1] = 0\n",
    "            elif s_tplus1[1] == \"Succes\" and (s_t[1] == 1000 or s_t[1] == 1250):\n",
    "                p_IM[s_t, s_tplus1] = 0.5\n",
    "            elif s_tplus1[1] == s_t[1] and s_t[1] == \"Succes\":\n",
    "                p_IM[s_t, s_tplus1] = 0.1\n",
    "            else: \n",
    "                p_IM[s_t, s_tplus1] = 0\n",
    "        else:\n",
    "            p_IM[s_t, s_tplus1] = 0\n",
    "\n",
    "# Consolidation of probabilities\n",
    "pTrans = {\"CM\":p_CM, \"IM\": p_IM}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e62ef",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Finally, to finish the basic definition of the problem, the rewards must be defined. As mentioned in the problem's definition, the only reward will be if the mission has succeded at the end of the immersion. Meaning, a reward of 1 will be recieved only at state (\"Succes\", 0).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5d10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rewards\n",
    "# Reward r(s_t)\n",
    "r = {}\n",
    "for s_t in S_t:\n",
    "    r[s_t] = 0\n",
    "\n",
    "r[(0,\"Succes\")] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62de68",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step of the Policy Iteration algorithm is initializing arbitrarily the Policy's Value for each state and the Policy. As opose to the policy used in *Policy Evaluation*, this one will be a *deterministic* policy. This means, that the action taken in a particular state will be certain.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af34a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbitrary Policy π\n",
    "pi = {}\n",
    "# The initialization will be taking calculated maneuvers in all states\n",
    "for s_t in S_t:\n",
    "    if s_t[0] != 0:\n",
    "        pi[s_t] = \"CM\"\n",
    "\n",
    "### Policy's Value\n",
    "# Initializing dictionary V for the value of the policy for each state in 0.5 (arbitrarily)\n",
    "V = {}\n",
    "for s_t in S_t:\n",
    "    V[s_t] = 0.5\n",
    "for samples in [0, 250, 500, 750, 1000, 1250, \"Succes\"]:\n",
    "    V[(0,samples)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa49529",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Then, the loop is coded. The threshold $\\theta$ will be the same for all iterations. Then, a boolean variable is declared, this variable will indicate if the current policy have or not a change that will improve it's performance. It is initialized in False so the loop can start. Inside the loop, first there will be a **Policy Iterative Evaluated** just as the one implemented previously. The only change is the adjustments for the *deterministic* and not *stochastic* policies. Afterwards, the **Policy Improvement** will be performed. This loop will continue until there is no improvements over the policy. Additionaly, a variable is declared to track how many iterations were made before achieving the optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218d19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations before achieving optimality: 4\n"
     ]
    }
   ],
   "source": [
    "### Policy Iteration\n",
    "# Threshold theta indicates the desired accuracy of the finded value throught iterations\n",
    "theta = 0.005\n",
    "\n",
    "# Policy_Stable indicates if the policy doesn't change in the current iteration\n",
    "Policy_Stable = False\n",
    "\n",
    "# Variable tracking the number of iterations\n",
    "num_iter = 0\n",
    "\n",
    "# The loop will be repeated until there is no improvement\n",
    "while not Policy_Stable:\n",
    "    \n",
    "    #### Policy Evaluation\n",
    "    # Initializing delta in 10 so the loop can start\n",
    "    delta = 10\n",
    "    # Iterations\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s_t in S_t:\n",
    "            if s_t[0] != 0:\n",
    "                v = V[s_t]\n",
    "                value = 0\n",
    "                for a in A:\n",
    "                    if pi[s_t] == a:\n",
    "                        for s_tplus1 in S_t:\n",
    "                            value += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1]) \n",
    "                V[s_t] = value\n",
    "                delta = max(delta, abs(v - V[s_t]))\n",
    "\n",
    "    #### Policy Improvement\n",
    "    Policy_Stable = True\n",
    "    for s_t in S_t:\n",
    "        if s_t[0] != 0:\n",
    "            old_action = pi[s_t]\n",
    "            arg = 0\n",
    "            argmax = \"Mission Failed\"\n",
    "            for a in A:\n",
    "                value = 0\n",
    "                for s_tplus1 in S_t:\n",
    "                    value += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1]) \n",
    "                if value > arg:\n",
    "                    arg = value\n",
    "                    argmax = a\n",
    "            if old_action != argmax:\n",
    "                Policy_Stable = False\n",
    "                pi[s_t] = argmax\n",
    "    \n",
    "    ### Update the number of iterations\n",
    "    num_iter += 1\n",
    "\n",
    "print(f\"Number of iterations before achieving optimality: {num_iter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd26e2c",
   "metadata": {},
   "source": [
    "Finally, the optimal policy has to be evaluated since in the las iteration the optimal policy was achieved but not evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f2e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Policy Evaluation for Optimal Policy\n",
    "# Initializing delta in 10 so the loop can start\n",
    "delta = 10\n",
    "# Iterations\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s_t in S_t:\n",
    "        if s_t[0] != 0:\n",
    "            v = V[s_t]\n",
    "            value = 0\n",
    "            for a in A:\n",
    "                if pi[s_t] == a:\n",
    "                    for s_tplus1 in S_t:\n",
    "                        value += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1]) \n",
    "            V[s_t] = value\n",
    "            delta = max(delta, abs(v - V[s_t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5d0a7",
   "metadata": {},
   "source": [
    "Policy Iteration has been completed. Now, specific values and decisions of the optimal policy can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ee5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal decision for the first 400 meters of immersion with 0 samples are:\n",
      "For 1000m: CM\n",
      "For 900m: CM\n",
      "For 700m: IM\n",
      "For 800m: IM\n",
      "(CM for Calculated Maneuver and IM for Improvised Maneuver)\n",
      "\n",
      "\n",
      "The probabilities of succes, following the optimal policy, at the middle of the immersion are:\n",
      "If there are 0 samples, the probablity is 0.501\n",
      "If there are 250 samples, the probablity is 0.652\n",
      "If there are 500 samples, the probablity is 0.812\n",
      "If there are 750 samples, the probablity is 0.901\n",
      "If there are 1000 samples, the probablity is 0.969\n",
      "If there are 1250 samples, the probablity is 0.99\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal decision for the first 400 meters of immersion with 0 samples are:\")\n",
    "for t in [1000, 900, 700, 800]:\n",
    "    print(f\"For {t}m: {pi[(t,0)]}\")\n",
    "print(\"(CM for Calculated Maneuver and IM for Improvised Maneuver)\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The probabilities of succes, following the optimal policy, at the middle of the immersion are:\")\n",
    "for samples in [0, 250, 500, 750, 1000, 1250]:\n",
    "    print(f\"If there are {samples} samples, the probablity is {round(V[(500,samples)],3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750b64b",
   "metadata": {},
   "source": [
    "## Problem 2: Aqueduct\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The same iterative process will be performed in this problem with some distinctions due to the characteristics of the problem. First, this is a *infinite* MDP, meaning the time steps have no valuable information. Considering this, states will contain only the liters of trash. Second, a discount factor will be applyied as stated in the description of the algorithm. On this terms, the basics of the problem will be coded in the next section: States, Transition Probabilities, Costs and the Discount Factor\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57206849",
   "metadata": {},
   "outputs": [],
   "source": [
    "### States\n",
    "S = list(range(3,11))\n",
    "\n",
    "### Decisions\n",
    "A = [\"Send\", \"Don't Send\"]\n",
    "\n",
    "### Transition probabilities\n",
    "# Send the cleaning team\n",
    "p_SEND = {}\n",
    "for s_t in S:\n",
    "    for s_tplus1 in S:\n",
    "        if s_tplus1 == 3:\n",
    "            p_SEND[s_t, s_tplus1] = 1\n",
    "        else:\n",
    "            p_SEND[s_t, s_tplus1] = 0\n",
    "\n",
    "# Dont's send the cleaning team \n",
    "p_DONTS = {}\n",
    "for s_t in S:\n",
    "    for s_tplus1 in S:\n",
    "        if s_tplus1 >= s_t:\n",
    "            p_DONTS[s_t, s_tplus1] = 1/(11-s_t)\n",
    "        else:\n",
    "            p_DONTS[s_t, s_tplus1] = 0\n",
    "\n",
    "# Consolidation of probabilities\n",
    "pTrans = {\"Send\": p_SEND, \"Don't Send\": p_DONTS}\n",
    "\n",
    "### Costs\n",
    "c = {}\n",
    "for s_t in S:\n",
    "    c[s_t, \"Send\"] = 12 + s_t * 0.01 * 200\n",
    "    c[s_t, \"Don't Send\"] =  s_t * 0.01 * 200\n",
    "\n",
    "### Discount Factor \n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff773cdd",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step of the Policy Iteration algorithm is initializing arbitrarily the Policy's Value for each state and the Policy. The Policy's Value will be arbitrarily set to 300 million pesos for all states. The Policy, on the other hand, will be initialized as sending the team in every state.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1687c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbitrary policy π\n",
    "pi = {}\n",
    "# The initialization will be sending the team in all states\n",
    "for s_t in S:\n",
    "    pi[s_t] = \"Send\"\n",
    "\n",
    "\n",
    "### Policy's Value\n",
    "V = {}\n",
    "# Initializing dictionary V for the value of the policy for each state in 300 (arbitrarily)\n",
    "for s_t in S:\n",
    "    V[s_t] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018664ce",
   "metadata": {},
   "source": [
    "Afterwards, **Policy Iteration** can be performed. The parameters for the loop will be the same as the ones used in the Problem 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d276f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations before achieving optimality: 3\n"
     ]
    }
   ],
   "source": [
    "### Policy Iteration\n",
    "# Threshold theta indicates the desired accuracy of the finded value throught iterations\n",
    "theta = 0.005\n",
    "\n",
    "# Policy_Stable indicates if the policy doesn't change in the current iteration\n",
    "Policy_Stable = False\n",
    "\n",
    "# Variable tracking the number of iterations\n",
    "num_iter = 0\n",
    "\n",
    "# The loop will be repeated until there is no improvement\n",
    "while not Policy_Stable:\n",
    "    \n",
    "    #### Policy Evaluation\n",
    "    # Initializing delta in 10 so the loop can start\n",
    "    delta = 10\n",
    "    # Iterations\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s_t in S:\n",
    "                v = V[s_t]\n",
    "                value = 0\n",
    "                for a in A:\n",
    "                    if pi[s_t] == a:\n",
    "                        for s_tplus1 in S:\n",
    "                            value += pTrans[a][s_t, s_tplus1] * (c[s_t,a] + gamma * V[s_tplus1]) \n",
    "                V[s_t] = value\n",
    "                delta = max(delta, abs(v - V[s_t]))\n",
    "\n",
    "    #### Policy Improvement\n",
    "    Policy_Stable = True\n",
    "    for s_t in S:\n",
    "        old_action = pi[s_t]\n",
    "        arg = 500\n",
    "        argmin = \"\"\n",
    "        for a in A:\n",
    "            value = 0\n",
    "            for s_tplus1 in S:\n",
    "                value += pTrans[a][s_t, s_tplus1] * (c[s_t, a] + gamma * V[s_tplus1]) \n",
    "            if value < arg:\n",
    "                arg = value\n",
    "                argmin = a\n",
    "        if old_action != argmin:\n",
    "            Policy_Stable = False\n",
    "            pi[s_t] = argmin\n",
    "    \n",
    "    ### Update the number of iterations\n",
    "    num_iter += 1\n",
    "\n",
    "print(f\"Number of iterations before achieving optimality: {num_iter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adaccd",
   "metadata": {},
   "source": [
    "Finally, the evaluation of the Optimal Policy will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcb9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Optimal Policy Evaluation\n",
    "# Initializing delta in 10 so the loop can start\n",
    "delta = 10\n",
    "# Iterations\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s_t in S:\n",
    "        v = V[s_t]\n",
    "        value = 0\n",
    "        for a in A:\n",
    "            if pi[s_t] == a:\n",
    "                for s_tplus1 in S:\n",
    "                    value += pTrans[a][s_t, s_tplus1] * (c[s_t,a] + gamma * V[s_tplus1]) \n",
    "        V[s_t] = value\n",
    "        delta = max(delta, abs(v - V[s_t])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7fa0",
   "metadata": {},
   "source": [
    "Now that Policy Improvement is done, the decision to make and expected cost for every state can be found. This, when following the optimal policy, of course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ae05af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal decision to make is:\n",
      "- If there are 3 liters of trash, Don't Send the Team\n",
      "- If there are 4 liters of trash, Don't Send the Team\n",
      "- If there are 5 liters of trash, Don't Send the Team\n",
      "- If there are 6 liters of trash, Send the Team\n",
      "- If there are 7 liters of trash, Send the Team\n",
      "- If there are 8 liters of trash, Send the Team\n",
      "- If there are 9 liters of trash, Send the Team\n",
      "- If there are 10 liters of trash, Send the Team\n",
      "\n",
      "\n",
      "The expected cost for every state is:\n",
      "- When there are 3 liters of trash: 298.468 Million Pesos\n",
      "- When there are 4 liters of trash: 301.743 Million Pesos\n",
      "- When there are 5 liters of trash: 304.923 Million Pesos\n",
      "- When there are 6 liters of trash: 307.545 Million Pesos\n",
      "- When there are 7 liters of trash: 309.545 Million Pesos\n",
      "- When there are 8 liters of trash: 311.545 Million Pesos\n",
      "- When there are 9 liters of trash: 313.545 Million Pesos\n",
      "- When there are 10 liters of trash: 315.545 Million Pesos\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal decision to make is:\")\n",
    "for s in S:\n",
    "    print(f\"- If there are {s} liters of trash, {pi[s]} the Team\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"The expected cost for every state is:\")\n",
    "for s in S:\n",
    "    print(f\"- When there are {s} liters of trash: {round(V[s],3)} Million Pesos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
