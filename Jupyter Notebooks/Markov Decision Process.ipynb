{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br>\n",
    "\n",
    "# Markov Decision Process (MDP)\n",
    "<br> <div style=\"text-align: justify\"> \n",
    " A Markov Decision Process (MDP) is defined as a *sequential decision problems under uncertainty* (Garcia & Rachelson, 2013), where a sequence of decisions must be taken in different moments by an agent/decision maker. The Markov principle applied in these decision processes states \"memory less\" throughout the decision process, meaning **\"Future is independent of the past given the present\"**. This property can be visualized as follows:\n",
    "</div> \n",
    "    \n",
    "$$ p[s_{t+1}|s_t] = p[s_{t+1}|s_1, s_2, ..., s_t]$$\n",
    "\n",
    "A MDP can be formulated as a 6-tuple ($S, A, T, p, r$), where:\n",
    "* $S_t$ is the state space in which the process takes place, at a given step $t$;\n",
    "* $A_t$ is the set of all possible actions which control the state's dynamics, at a given step $t$;\n",
    "* $T$ is the set of time steps where decisions need to be made;\n",
    "* $p()$ denotes the state transition probability function;\n",
    "* $r_t()$ provides the reward function defined on state transitions, given a step $t$;\n",
    "\n",
    "<br> <div style=\"text-align: justify\">\n",
    "A decision will be taken at the step $t$ and state $s_t$, to arrive to the state $s_{t+1}$ with the asociated probability $p[s_{t+1}|s_t]$, hence the stochastic nature of the process. It must be noted that this probability may also depend on the decision/action taken in the current state  $p[s_{t+1}|s_t, a]  (a\\in A_t) $. Due to the fact that $p()$ are probabilites, the condition $\\sum_{s_{t+1}\\in{S_{t+1}}}{p[s_{t+1}|s_t,a]=1}\\space\\forall s_t\\in S_t, a_t\\in A_t$ must be satisfyied. Therefore, the matrix $P_a$ with components $p(s_t, s_{t+1})$ indicating the probability of going to the state $s_t\\in S_t$ to the state $s_{t+1}\\in S_{t+1}$ when taking the decision $a\\in A_t$ is a *stochastc* matrix. Another important component of MDP's are the rewards $r_{t+1} = r(s_t,a,s_{t+1})$ obtained by taking a desicion $a$ in a given state $s_t$ at step $t$ and arriving to state $s_{t+1}$ at step $t+1$. When the probability and reward functions are the same troughout the decision steps, they are said to be *stationary*, which means $p_t()=p()$ and $r_t()=r()$. A usefull visualization of a MDP would be\n",
    "</div> \n",
    "    \n",
    "$$\n",
    "s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3...\n",
    "$$\n",
    "\n",
    "<br> <div style=\"text-align: justify\">\n",
    "With all these components, we can state the relevant nature of a Markovian Decision Process. Throughout the decision horizion, the goal will be to maximize the overall revenue by taking decisions. Given the stochastic component, the solution to an MDP will be an optimal *policy*, $\\pi$, wich will state the ideal decision to make in every state in search of the maximum overall performance.\n",
    "</div>\n",
    "    \n",
    "<br> <div style=\"text-align: justify\">\n",
    "A decision proces can have an finite or infinite decision horizon, where it will be defined as a *finite MPD* or *infinite MDP*, respectively. When working with a *finite* MDP with N seteps, the rewards collected throughout the decision process will add up to a expected Value. The Value for an specific policy $\\pi$, starting state $s$ can be found as:\n",
    "</div> \n",
    "    \n",
    "$$\n",
    "V_N^{\\pi}(s)=E^\\pi \\left[\\sum_{t=0}^{N}{r_{t+1}|s_0=s} \\right]\\space \\space \\space \\forall \\space \\space s\\in S_0\n",
    "$$\n",
    "\n",
    "<br> <div style=\"text-align: justify\">\n",
    "For *infinite* MPD's, a new concept defined as *discont factor*, $\\gamma$, is introduced. Due to the lack of clear terminal state of an infinite MDP, a discount factor is used to specify how much importance is given to a future reward in comparison to a immediate reward. This factor can take values between 0 and 1, where when chosing high values give more importance to future reward than with low factor values. With the new concept of *discont factor*, The Value for an specific starting state $s$ at a given step $t$ can be found as: \n",
    "</div>\n",
    "    \n",
    "$$\n",
    "V^{\\pi}(s)=E^\\pi \\left[\\sum_{t=0}^{âˆž}{\\gamma ^tr_{t+1}|s_0=s} \\right]\\space \\space \\space \\forall \\space \\space s\\in S_0\n",
    "$$\n",
    "\n",
    "  \n",
    "**Bibliography**\n",
    "\n",
    "Garcia, F., & Rachelson, E. (2013). Markov decision processes. *Markov Decision Processes in Artificial Intelligence*, 1-38."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
