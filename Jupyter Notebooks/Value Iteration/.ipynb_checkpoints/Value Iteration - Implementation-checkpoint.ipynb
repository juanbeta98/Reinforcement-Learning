{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb6bec8",
   "metadata": {},
   "source": [
    "## Problem 1: Sample Recolection\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step to applying the Policy Iteration Algorithm to this problem will be coding all of the initial conditions of the problem. We start by coding the time steps, the possible number of samples and the possible actions to be taken. Due to the fact that this is a finite, *non-stationary* MDP, combinations of time steps and number of samples will be considered the states of the problem. For example, 1250 samples at 500m of immersion will be an state and 1250 samples at 400m of immersion will be another state. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1173253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time steps of decision\n",
    "T = [1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0]\n",
    "\n",
    "### Samples\n",
    "Samples = [0, 250, 500, 750, 1000, 1250, \"Succes\"]\n",
    "\n",
    "### Space of States\n",
    "S_t = []\n",
    "for S in [0, 250, 500, 750]:\n",
    "    S_t.append((1000,S))\n",
    "for t in [900, 800, 700, 600, 500, 400, 300, 200, 100, 0]:\n",
    "    for S in Samples:\n",
    "        S_t.append((t,S))\n",
    "\n",
    "### Actions \n",
    "# CM for Calculated Maneuver\n",
    "# IM for improvised Maneuver \n",
    "A = [\"CM\", \"IM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049307a",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Next, transition probabilities will be coded. As mention before, this probabilities will indicate the stochastic nature of the problem on the transitions between states. As the states contain steps of decision and number of samples, this information will be taken into account in the transition probabilities. Also, the probabilities depend on the action that is performed. Therefore, there will be to dictionaries of probabilites, one for Calculated Maneuvers and one for Improvised Maneuvers. At the end, both of this dictionaries will be consolidated in one dictionary. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129f97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transition probabilities\n",
    "## Transition Probabilites will be defined as dictionaries\n",
    "# Transition Probabilities for Calculated Maneuvers\n",
    "p_CM = {}\n",
    "for s_t in S_t:\n",
    "    for s_tplus1 in S_t:\n",
    "        if s_tplus1[0] == s_t[0] - 100:\n",
    "            if s_t[1] != \"Succes\" and s_tplus1[1] != \"Succes\":\n",
    "                if s_tplus1[1] == s_t[1] + 250 and s_t[1] < 1250:\n",
    "                    p_CM[s_t, s_tplus1] = 0.3\n",
    "                elif s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:\n",
    "                    p_CM[s_t, s_tplus1] = 0.2\n",
    "                elif s_tplus1[1] == s_t[1] + 750 and s_t[1] < 750:\n",
    "                    p_CM[s_t, s_tplus1] = 0.1\n",
    "                elif s_tplus1[1] == s_t[1] and s_t[1] != \"Succes\":\n",
    "                    p_CM[s_t, s_tplus1] = 0.4\n",
    "                else: \n",
    "                    p_CM[s_t, s_tplus1] = 0\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 1000:\n",
    "                p_CM[s_t, s_tplus1] = 0.3\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 1250:\n",
    "                p_CM[s_t, s_tplus1] = 0.6\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 750:\n",
    "                p_CM[s_t, s_tplus1] = 0.1\n",
    "            elif s_tplus1[1] == s_t[1] and s_t[1] == \"Succes\":\n",
    "                p_CM[s_t, s_tplus1] = 1\n",
    "            else: \n",
    "                p_CM[s_t, s_tplus1] = 0\n",
    "        else:\n",
    "            p_CM[s_t, s_tplus1] = 0\n",
    "    \n",
    "# Transition Probabilites for Improvised Maneuvers\n",
    "p_IM = {}\n",
    "for s_t in S_t:\n",
    "    for s_tplus1 in S_t:\n",
    "        if s_tplus1[0] == s_t[0] - 100:\n",
    "            if s_t[1] != \"Succes\" and s_tplus1[1] != \"Succes\":\n",
    "                if s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:\n",
    "                    p_IM[s_t, s_tplus1] = 0.5\n",
    "                elif s_tplus1[1] == s_t[1] and s_t[1] <= 1250:\n",
    "                    p_IM[s_t, s_tplus1] = 0.5\n",
    "                else: \n",
    "                    p_IM[s_t, s_tplus1] = 0\n",
    "            elif s_tplus1[1] == \"Succes\" and (s_t[1] == 1000 or s_t[1] == 1250):\n",
    "                p_IM[s_t, s_tplus1] = 0.5\n",
    "            elif s_tplus1[1] == s_t[1] and s_t[1] == \"Succes\":\n",
    "                p_IM[s_t, s_tplus1] = 0.1\n",
    "            else: \n",
    "                p_IM[s_t, s_tplus1] = 0\n",
    "        else:\n",
    "            p_IM[s_t, s_tplus1] = 0\n",
    "\n",
    "# Consolidation of probabilities\n",
    "pTrans = {\"CM\":p_CM, \"IM\": p_IM}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ad9d5",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Finally, to finish the basic definition of the problem, the rewards must be defined. As mentioned in the problem's definition, the only reward will be if the mission has succeded at the end of the immersion. Meaning, a reward of 1 will be recieved only at state (\"Succes\", 0).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f962fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rewards\n",
    "# Reward r(s_t)\n",
    "r = {}\n",
    "for s_t in S_t:\n",
    "    r[s_t] = 0\n",
    "\n",
    "r[(0,\"Succes\")] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8a93b",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step of Value Iteration algorithm is initializing arbitrarily the Policy's Value for each state and the Policy. As opose to the policy used in *Policy Evaluation*, this one will be a *deterministic* policy. This means, that the action taken in a particular state will be certain. Also, all of the terminal states will be assigned a Value of 0.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59871582",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbitrary Policy π\n",
    "pi = {}\n",
    "# The initialization will be taking calculated maneuvers in all states\n",
    "for s_t in S_t:\n",
    "    if s_t[0] != 0:\n",
    "        pi[s_t] = \"CM\"\n",
    "\n",
    "### Policy's Value\n",
    "# Defining dictionary V for the value of the policy for each state in 0.5 (arbitrarily)\n",
    "V = {}\n",
    "for s_t in S_t:\n",
    "    V[s_t] = 0.5\n",
    "for samples in [0, 250, 500, 750, 1000, 1250, \"Succes\"]:\n",
    "    V[(0,samples)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2c459",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Then, the loop is coded. The threshold $\\theta$ will be the same for all iterations. The loop is quite similar to the **Policy Evaluation** with the difference of the optimal decision chosing. On each loop, the decision that maximizes the value will be stored and the value it provides will be assigned as the Value of the Policy for that state. Additionaly, a variable is declared to track how many iterations were made before achieving the optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf39d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Value Iteration\n",
    "# Threshold theta indicates the desired accuracy of the finded value throught iterations\n",
    "theta = 0.005\n",
    "\n",
    "# Variable tracking the number of iterations\n",
    "num_iter = 0\n",
    "\n",
    "# Initializing delta in 10 so the loop can start\n",
    "delta = 10\n",
    "\n",
    "# Iterations\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s_t in S_t:\n",
    "        if s_t[0] != 0:\n",
    "            v = V[s_t]\n",
    "            Vmax = 0\n",
    "            argmax = \"\"\n",
    "            for a in A:\n",
    "                value = 0\n",
    "                for s_tplus1 in S_t:\n",
    "                    value += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1]) \n",
    "                if value > Vmax:\n",
    "                    Vmax = value\n",
    "                    argmax = a\n",
    "            V[s_t] = Vmax\n",
    "            pi[s_t] = argmax\n",
    "            delta = max(delta, abs(v - V[s_t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780ee24",
   "metadata": {},
   "source": [
    "Value Iteration has been completed. Now, specific values and decisions of the optimal policy can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec15646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal decision for 1000 meters when having 0 samples is: CM\n",
      "The optimal decision for 900 meters when having 0 samples is: CM\n",
      "The optimal decision for 800 meters when having 0 samples is: IM\n",
      "The optimal decision for 700 meters when having 0 samples is: IM\n",
      "The optimal decision for 600 meters when having 0 samples is: IM\n",
      "The optimal decision for 500 meters when having 0 samples is: IM\n",
      "The optimal decision for 400 meters when having 0 samples is: IM\n",
      "The optimal decision for 300 meters when having 0 samples is: IM\n",
      "The optimal decision for 200 meters when having 0 samples is: CM\n",
      "The optimal decision for 100 meters when having 0 samples is: Mission Failed\n",
      "\n",
      "\n",
      "The probability of succes at the beggining of the mission is:0.947\n"
     ]
    }
   ],
   "source": [
    "for t in T:\n",
    "    if t != 0:\n",
    "        if pi[(t,0)] == \"\":\n",
    "            pi[(t,0)] = \"Mission Failed\"\n",
    "        print(f\"The optimal decision for {t} meters when having 0 samples is: {pi[(t,0)]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The probability of succes at the beggining of the mission is:{round(V[(1000,0)],3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c3bea",
   "metadata": {},
   "source": [
    "## Problem 2: Aqueduct\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The same iterative process will be performed in this problem with some distinctions due to the characteristics of the problem. First, this is a *infinite* MDP, meaning the time steps have no valuable information. Considering this, states will contain only the liters of trash. Second, a discount factor will be applyied as stated in the description of the algorithm. On this terms, the basics of the problem will be coded in the next section: States, Transition Probabilities, Costs and the Discount Factor\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7145fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### States\n",
    "S = list(range(3,11))\n",
    "\n",
    "### Decisions\n",
    "A = [\"Send\", \"Don't Send\"]\n",
    "\n",
    "### Transition probabilities\n",
    "# Send the cleaning team\n",
    "p_SEND = {}\n",
    "for s_t in S:\n",
    "    for s_tplus1 in S:\n",
    "        if s_tplus1 == 3:\n",
    "            p_SEND[s_t, s_tplus1] = 1\n",
    "        else:\n",
    "            p_SEND[s_t, s_tplus1] = 0\n",
    "\n",
    "# Dont's send the cleaning team \n",
    "p_DONTS = {}\n",
    "for s_t in S:\n",
    "    for s_tplus1 in S:\n",
    "        if s_tplus1 >= s_t:\n",
    "            p_DONTS[s_t, s_tplus1] = 1/(11-s_t)\n",
    "        else:\n",
    "            p_DONTS[s_t, s_tplus1] = 0\n",
    "\n",
    "# Consolidation of probabilities\n",
    "pTrans = {\"Send\": p_SEND, \"Don't Send\": p_DONTS}\n",
    "\n",
    "### Costs\n",
    "c = {}\n",
    "for s_t in S:\n",
    "    c[s_t, \"Send\"] = 12 + s_t * 0.01 * 200\n",
    "    c[s_t, \"Don't Send\"] =  s_t * 0.01 * 200\n",
    "\n",
    "### Discount Factor \n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af465654",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step of the Value Iteration algorithm is initializing arbitrarily the Policy's Value for each state and the Policy. The Policy's Value will be arbitrarily set to 300 million pesos for all states. The Policy, on the other hand, will be initialized as sending the team in every state. As there are no terminal states, there will be no assigments of 0. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77a6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbitrary policy π\n",
    "pi = {}\n",
    "# The initialization will be sending the team in all states\n",
    "for s_t in S:\n",
    "    pi[s_t] = \"Send\"\n",
    "\n",
    "\n",
    "### Policy's Value\n",
    "V = {}\n",
    "# Initializing dictionary V for the value of the policy for each state in 300 (arbitrarily)\n",
    "for s_t in S:\n",
    "    V[s_t] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f31ca",
   "metadata": {},
   "source": [
    "Afterwards, **Value Iteration** can be performed. The parameters for the loop will be the same as the ones used in the Problem 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb0d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Value Iteration\n",
    "# Threshold theta indicates the desired accuracy of the finded value throught iterations\n",
    "theta = 0.005\n",
    "\n",
    "# Variable tracking the number of iterations\n",
    "num_iter = 0\n",
    "\n",
    "# Iterations\n",
    "# Initializing delta in 10 so the loop can start\n",
    "delta = 10\n",
    "# Iterations\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s_t in S:\n",
    "            v = V[s_t]\n",
    "            Vmin = 500\n",
    "            argmin = \"\"\n",
    "            for a in A:\n",
    "                value = 0\n",
    "                for s_tplus1 in S:\n",
    "                    value += pTrans[a][s_t, s_tplus1] * (c[s_t,a] + gamma * V[s_tplus1]) \n",
    "                if value < Vmin:\n",
    "                    Vmin = value\n",
    "                    argmin = a\n",
    "            V[s_t] = Vmin\n",
    "            pi[s_t] = argmin\n",
    "            delta = max(delta, abs(v - V[s_t]))\n",
    "\n",
    "    ### Update the number of iterations\n",
    "    num_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95793a8",
   "metadata": {},
   "source": [
    "Now that Policy Improvement is done, the decision to make and expected cost for every state can be found. This, when following the optimal policy, of course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6947a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations before achieving optimality: 63\n",
      "The optimal decision to make is:\n",
      "- If there are 3 liters of trash, Don't Send the Team\n",
      "- If there are 4 liters of trash, Don't Send the Team\n",
      "- If there are 5 liters of trash, Don't Send the Team\n",
      "- If there are 6 liters of trash, Send the Team\n",
      "- If there are 7 liters of trash, Send the Team\n",
      "- If there are 8 liters of trash, Send the Team\n",
      "- If there are 9 liters of trash, Send the Team\n",
      "- If there are 10 liters of trash, Send the Team\n",
      "\n",
      "\n",
      "The expected cost for every state is:\n",
      "- When there are 3 liters of trash: 298.367 Million Pesos\n",
      "- When there are 4 liters of trash: 301.642 Million Pesos\n",
      "- When there are 5 liters of trash: 304.823 Million Pesos\n",
      "- When there are 6 liters of trash: 307.449 Million Pesos\n",
      "- When there are 7 liters of trash: 309.449 Million Pesos\n",
      "- When there are 8 liters of trash: 311.449 Million Pesos\n",
      "- When there are 9 liters of trash: 313.449 Million Pesos\n",
      "- When there are 10 liters of trash: 315.449 Million Pesos\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of iterations before achieving optimality: {num_iter}\") \n",
    "\n",
    "print(\"The optimal decision to make is:\")\n",
    "for s in S:\n",
    "    print(f\"- If there are {s} liters of trash, {pi[s]} the Team\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"The expected cost for every state is:\")\n",
    "for s in S:\n",
    "    print(f\"- When there are {s} liters of trash: {round(V[s],3)} Million Pesos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
