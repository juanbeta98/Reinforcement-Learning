{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import support_modules as sm\n",
    "import numpy as np\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot\n",
    "\n",
    "## Description\n",
    "<div style=\"text-align: justify\">    \n",
    "The Acrobot environment is based on Sutton’s work in “Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding” and Sutton and Barto’s book. The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards.\n",
    "\n",
    "As seen in the Gif: two blue links connected by two green joints. The joint in between the two links is actuated. The goal is to swing the free end of the outer-link to reach the target height (black horizontal line above system) by applying torque on the actuator.\n",
    "</div>\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/acrobot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      7\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m----> 8\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     11\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/SCAEnv/lib/python3.9/site-packages/gymnasium/core.py:418\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/SCAEnv/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/SCAEnv/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:65\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/SCAEnv/lib/python3.9/site-packages/gymnasium/envs/classic_control/acrobot.py:367\u001b[0m, in \u001b[0;36mAcrobotEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -498.778\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1',render_mode=None)\n",
    "\n",
    "rewards = list()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "    \n",
    "    rewards.append(ep_reward)\n",
    "\n",
    "env.close()\n",
    "print(f'Average reward: {sum(rewards)/len(rewards)}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Parameters\n",
    "LEARNING_RATE = 0.1         # How fast does the agent learn\n",
    "DISCOUNT = 0.95             # How important are future actions\n",
    "\n",
    "EPISODES = 10000            # Number of episodes\n",
    "\n",
    "epsilon = 0.5                           # Exploration rate\n",
    "START_EPSILON_DECAYING = 1              # First episode at which decay epsilon\n",
    "END_EPSILON_DECAYING = 1250             # Last episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d0891217aa48fc8482d3cecdc90303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2500, description='Episodes', max=10000, min=50, step=50)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b38f28130414b139abe9d8640f22362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, description='Learning r.', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23bfdc9dce640bb9a2a829f4bd2929e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.95, description='Discount f.', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2b237fb4024e8d8eace778b775ea32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Exploration r.', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flexible Parameters (ipywidgets)\n",
    "EPISODES_W = widgets.IntSlider(value = 2500, min = 50, max = 10000, step = 50, description = 'Episodes')\n",
    "\n",
    "LEARNING_RATE_W = widgets.FloatSlider(value = 0.1, min = 0, max = 1, step = 0.05, description = 'Learning r.')\n",
    "DISCOUNT_W = widgets.FloatSlider(value = 0.95, min = 0, max = 1, step = 0.05, description = 'Discount f.')\n",
    "\n",
    "epsilon_W = widgets.FloatSlider(value = 0.5, min = 0, max = 1, step = 0.05, description = 'Exploration r.')\n",
    "\n",
    "display(EPISODES_W,LEARNING_RATE_W,DISCOUNT_W,epsilon_W)\n",
    "\n",
    "START_EPSILON_DECAYING = 1                      # First episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "\n",
    "discrete_partitions = 30\n",
    "\n",
    "# Retrieving \n",
    "EPISODES = EPISODES_W.value; LEARNING_RATE = LEARNING_RATE_W.value\n",
    "DISCOUNT = DISCOUNT_W.value; epsilon = epsilon_W.value\n",
    "epsilon_decaying_value = epsilon / ((EPISODES//1.5) - START_EPSILON_DECAYING)     # Amount of decayment of epsilon    \n",
    "\n",
    "# Generate the discrete state space and the interval of each discrete space\n",
    "discrete_state_space,discrete_state_intervals = sm.Q_Learning_Agent.generate_discrete_states(discrete_partitions,env)\n",
    "\n",
    "# Generate the q_table\n",
    "q_table = sm.Q_Learning_Agent.generate_q_table('random',env,discrete_state_space,low=-2,high=0)\n",
    "\n",
    "# Rewards\n",
    "ep_rewards = list()\n",
    "success = list()\n",
    "epsilons = list()\n",
    "\n",
    "\n",
    "### Training\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)        # Discrete initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:                    # Randomize actions with epsilon\n",
    "            action = np.argmax(q_table[discrete_state])     # Action taken from the argmax of the current state\n",
    "        else:\n",
    "            action = env.action_space.sample()              # Action taken ramdomly\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)       # Retrieve information\n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = sm.Q_Learning_Agent.get_discrete_state(new_state,env,discrete_state_intervals,discrete_partitions)  # Discretize new state\n",
    "        \n",
    "        if not done: \n",
    "            q_table = sm.Q_Learning_Agent.update_q_table(q_table,discrete_state,new_discrete_state,action,reward,LEARNING_RATE,DISCOUNT)\n",
    "        \n",
    "        elif terminated:\n",
    "            q_table[discrete_state + (action, )] = 0        # Update value when goal is reached\n",
    "        \n",
    "        discrete_state = new_discrete_state                 # Update state\n",
    "    \n",
    "    epsilon = sm.Q_Learning_Agent.decay_epsilon(epsilon,episode,epsilon_decaying_value,START_EPSILON_DECAYING,EPISODES//1.5)\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    success.append(terminated)\n",
    "    epsilons.append(epsilon)\n",
    "     \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing training results\n",
    "sm.visualizations.plot_moving_average([success],['Q-Learning'],50,\n",
    "                                      title='Performance of Q-Learning Agent (Success)',ylabel='Success rate')\n",
    "sm.visualizations.plot_moving_average([ep_rewards],['Q-Learning'],50,\n",
    "                                      title='Performance of Q-Learning Agent (Rewards)',ylabel='Rewards')\n",
    "# sm.visualizations.plot_series([epsilons],['epsilon'],title='Exploration Rate Decaying', ylabel='epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Agent on 100 New Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "success = list()\n",
    "rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "    success.append(terminated)\n",
    "    rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of Q-Learning Agent on 100 episodes: {round(sum(success)/len(success),2)}')\n",
    "print(f'Average rewards of Q-Learning Agent on 100 episodes: {round(sum(rewards)/len(rewards),2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
