{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "import support_modules as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BETA] Lunar Lander\n",
    "\n",
    "## Description\n",
    "\n",
    "<div style=\"text-align: justify\">    \n",
    "This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
    "</div>\n",
    "\n",
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False -100\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "state, _ = env.reset()  \n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()\n",
    "print(terminated,truncated,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.0\n",
      "Average reward: -182.05876755751794\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode=None)\n",
    "\n",
    "rewards = list()\n",
    "success = list()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "    \n",
    "    success.append(ep_reward>=200)\n",
    "    rewards.append(ep_reward)\n",
    "    \n",
    "\n",
    "env.close()\n",
    "print(f'Success rate: {sum(success)/len(success)}')\n",
    "print(f'Average reward: {sum(rewards)/len(rewards)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529eb11b10554fdcb386a99f076ea622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=25000, description='Number Episodes', layout=Layout(width='40%'), max=50000, min=50, step=50, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c86b8502484430917fef0508865777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, description='Learning rate', layout=Layout(width='40%'), max=1.0, step=0.05, style=Slid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcaa74cc1c64d64b12be5e503749059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.95, description='Discount factor', layout=Layout(width='40%'), max=1.0, step=0.05, style=S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8929a23c7a514e9a973a957ac0fc3652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.8, description='Exploration rate', layout=Layout(width='40%'), max=1.0, step=0.05, style=S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flexible Parameters (ipywidgets)\n",
    "EPISODES_W = widgets.IntSlider(value=25000, min=50, max=50000, step=50, description='Number Episodes', \n",
    "                               style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "LEARNING_RATE_W = widgets.FloatSlider(value=0.1, min=0, max=1, step=0.05, description='Learning rate',\n",
    "                                      style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "DISCOUNT_W = widgets.FloatSlider(value=0.95, min=0, max=1, step=0.05, description='Discount factor',\n",
    "                                 style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "epsilon_W = widgets.FloatSlider(value=0.8, min=0, max=1, step=0.05, description='Exploration rate',\n",
    "                                style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "display(EPISODES_W,LEARNING_RATE_W,DISCOUNT_W,epsilon_W)\n",
    "\n",
    "START_EPSILON_DECAYING = 1                      # First episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 5 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m new_discrete_state \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mQ_Learning_Agent\u001b[38;5;241m.\u001b[39mget_discrete_state(new_state,env,discrete_state_intervals,discrete_partitions)  \u001b[38;5;66;03m# Discretize new state\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done: \n\u001b[0;32m---> 45\u001b[0m     q_table \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_Learning_Agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_q_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdiscrete_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_discrete_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDISCOUNT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m episode_reward\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     48\u001b[0m     q_table[discrete_state \u001b[38;5;241m+\u001b[39m (action, )] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m        \u001b[38;5;66;03m# Update value when goal is reached\u001b[39;00m\n",
      "File \u001b[0;32m~/My Drive/Research/Reinforcement Learning/Reinforcement-Learning/Jupyter Notebooks/OpenAI Gymnasium/Classic Control/../support_modules.py:142\u001b[0m, in \u001b[0;36mQ_Learning_Agent.update_q_table\u001b[0;34m(q_table, state, new_state, action, reward, LEARNING_RATE, DISCOUNT)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_q_table\u001b[39m(q_table: np\u001b[38;5;241m.\u001b[39mndarray, state: \u001b[38;5;28mtuple\u001b[39m, new_state: \u001b[38;5;28mtuple\u001b[39m, action: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    126\u001b[0m                 reward: \u001b[38;5;28mfloat\u001b[39m, LEARNING_RATE: \u001b[38;5;28mfloat\u001b[39m, DISCOUNT: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Update the Q-table based on the observed reward and the transition to a new state.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    - q_table (np.ndarray): The updated Q-table.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     max_future_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Maximum Q-value for the new state\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     current_q \u001b[38;5;241m=\u001b[39m q_table[state \u001b[38;5;241m+\u001b[39m (action,)]     \u001b[38;5;66;03m# Q-value for the current state-action pair\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Update the Q-value for the current state-action pair using the Q-learning formula\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 5 with size 10"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=None)\n",
    "\n",
    "discrete_partitions = 10\n",
    "\n",
    "# Retrieving \n",
    "EPISODES = EPISODES_W.value; LEARNING_RATE = LEARNING_RATE_W.value\n",
    "DISCOUNT = DISCOUNT_W.value; epsilon = epsilon_W.value\n",
    "epsilon_decaying_value = epsilon / ((EPISODES//1.5) - START_EPSILON_DECAYING)     # Amount of decayment of epsilon    \n",
    "\n",
    "# Generate the discrete state space and the interval of each discrete space \n",
    "discrete_state_space,discrete_state_intervals = sm.Q_Learning_Agent.generate_discrete_states(discrete_partitions,env)\n",
    "\n",
    "# Generate the q_table \n",
    "q_table = sm.Q_Learning_Agent.generate_q_table('init_value',env,discrete_state_space,value=0)\n",
    "\n",
    "# Rewards\n",
    "ep_rewards = list()\n",
    "success = list()\n",
    "epsilons = list()\n",
    "\n",
    "\n",
    "### Training\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)        # Discrete initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:                    # Randomize actions with epsilon\n",
    "            action = np.argmax(q_table[discrete_state])     # Action taken from the argmax of the current state\n",
    "        else:\n",
    "            action = env.action_space.sample()              # Action taken at random\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)       # Retrieve information\n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = sm.Q_Learning_Agent.get_discrete_state(new_state,env,discrete_state_intervals,discrete_partitions)  # Discretize new state\n",
    "        \n",
    "        if not done: \n",
    "            q_table = sm.Q_Learning_Agent.update_q_table(q_table,discrete_state,new_discrete_state,action,reward,LEARNING_RATE,DISCOUNT)\n",
    "        \n",
    "        elif episode_reward>=200:\n",
    "            q_table[discrete_state + (action, )] = 200        # Update value when goal is reached\n",
    "        \n",
    "        discrete_state = new_discrete_state                 # Update state\n",
    "    \n",
    "    epsilon = sm.Q_Learning_Agent.decay_epsilon(epsilon,episode,epsilon_decaying_value,START_EPSILON_DECAYING,EPISODES//1.5)\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    success.append(episode_reward>=200)\n",
    "    epsilons.append(epsilon)\n",
    "     \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
