{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "import support_modules as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car \n",
    "\n",
    "## Description\n",
    "\n",
    "<div style=\"text-align: justify\">    \n",
    "The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. There are two versions of the mountain car domain in gymnasium: one with discrete actions and one with continuous. This version is the one with discrete actions.\n",
    "</div>\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/mountain_car/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0',render_mode=None)\n",
    "\n",
    "rewards = list()\n",
    "success = list()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "    \n",
    "    rewards.append(ep_reward)\n",
    "    success.append(terminated)\n",
    "\n",
    "env.close()\n",
    "print(f'Average reward: {sum(rewards)/len(rewards)}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.visualizations.plot_moving_average([success],['Random Policy'],20,\n",
    "                                      title='Performance of random policy', ylabel='Success rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Parameters\n",
    "LEARNING_RATE = 0.1         # How fast does the agent learn\n",
    "DISCOUNT = 0.95             # How important are future actions\n",
    "\n",
    "EPISODES = 10000            # Number of episodes\n",
    "\n",
    "epsilon = 0.5                           # Exploration rate\n",
    "START_EPSILON_DECAYING = 1              # First episode at which decay epsilon\n",
    "END_EPSILON_DECAYING = 1250             # Last episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible Parameters (ipywidgets)\n",
    "EPISODES_W = widgets.IntSlider(value = 2500, min = 50, max = 10000, step = 50, description = 'Episodes')\n",
    "\n",
    "LEARNING_RATE_W = widgets.FloatSlider(value = 0.1, min = 0, max = 1, step = 0.05, description = 'Learning r.')\n",
    "DISCOUNT_W = widgets.FloatSlider(value = 0.95, min = 0, max = 1, step = 0.05, description = 'Discount f.')\n",
    "\n",
    "epsilon_W = widgets.FloatSlider(value = 0.5, min = 0, max = 1, step = 0.05, description = 'Exploration r.')\n",
    "\n",
    "display(EPISODES_W,LEARNING_RATE_W,DISCOUNT_W,epsilon_W)\n",
    "\n",
    "START_EPSILON_DECAYING = 1                      # First episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=None)\n",
    "\n",
    "discrete_partitions = 30\n",
    "\n",
    "# Retrieving \n",
    "EPISODES = EPISODES_W.value; LEARNING_RATE = LEARNING_RATE_W.value\n",
    "DISCOUNT = DISCOUNT_W.value; epsilon = epsilon_W.value\n",
    "epsilon_decaying_value = epsilon / ((EPISODES//1.5) - START_EPSILON_DECAYING)     # Amount of decayment of epsilon    \n",
    "\n",
    "# Generate the discrete state space and the interval of each discrete space \n",
    "discrete_state_space,discrete_state_intervals = sm.Q_Learning_Agent.generate_discrete_states(discrete_partitions,env)\n",
    "\n",
    "# Generate the q_table \n",
    "q_table = sm.Q_Learning_Agent.generate_q_table('random',env,discrete_state_space,low=-2,high=0)\n",
    "\n",
    "# Rewards\n",
    "ep_rewards = list()\n",
    "success = list()\n",
    "epsilons = list()\n",
    "\n",
    "\n",
    "### Training\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)        # Discrete initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:                    # Randomize actions with epsilon\n",
    "            action = np.argmax(q_table[discrete_state])     # Action taken from the argmax of the current state\n",
    "        else:\n",
    "            action = env.action_space.sample()              # Action taken ramdomly\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)       # Retrieve information\n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = sm.Q_Learning_Agent.get_discrete_state(new_state,env,discrete_state_intervals,discrete_partitions)  # Discretize new state\n",
    "        \n",
    "        if not done: \n",
    "            q_table = sm.Q_Learning_Agent.update_q_table(q_table,discrete_state,new_discrete_state,action,reward,LEARNING_RATE,DISCOUNT)\n",
    "        \n",
    "        elif terminated:\n",
    "            q_table[discrete_state + (action, )] = 0        # Update value when goal is reached\n",
    "        \n",
    "        discrete_state = new_discrete_state                 # Update state\n",
    "    \n",
    "    epsilon = sm.Q_Learning_Agent.decay_epsilon(epsilon,episode,epsilon_decaying_value,START_EPSILON_DECAYING,EPISODES//1.5)\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    success.append(terminated)\n",
    "    epsilons.append(epsilon)\n",
    "     \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing training results\n",
    "sm.visualizations.plot_moving_average([success],['Q-Learning'],50,\n",
    "                                      title='Performance of Q-Learning Agent (Success)',ylabel='Success rate')\n",
    "sm.visualizations.plot_moving_average([ep_rewards],['Q-Learning'],50,\n",
    "                                      title='Performance of Q-Learning Agent (Rewards)',ylabel='Rewards')\n",
    "# sm.visualizations.plot_series([epsilons],['epsilon'],title='Exploration Rate Decaying', ylabel='epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Agent on 100 New Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=None)\n",
    "success = list()\n",
    "rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "    success.append(terminated)\n",
    "    rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of Q-Learning Agent on 100 episodes: {round(sum(success)/len(success),2)}')\n",
    "print(f'Average rewards of Q-Learning Agent on 100 episodes: {round(sum(rewards)/len(rewards),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
