{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "import support_modules as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot\n",
    "\n",
    "## Description\n",
    "<div style=\"text-align: justify\">    \n",
    "The Acrobot environment is based on Sutton’s work in “Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding” and Sutton and Barto’s book. The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards.\n",
    "\n",
    "As seen in the Gif: two blue links connected by two green joints. The joint in between the two links is actuated. The goal is to swing the free end of the outer-link to reach the target height (black horizontal line above system) by applying torque on the actuator.\n",
    "</div>\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/acrobot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1',render_mode=None)\n",
    "\n",
    "rewards = list()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "    \n",
    "    rewards.append(ep_reward)\n",
    "\n",
    "env.close()\n",
    "print(f'Success rate: {sum(success)/len(success)}')\n",
    "print(f'Average reward: {sum(rewards)/len(rewards)}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Beta] Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Parameters\n",
    "LEARNING_RATE = 0.1         # How fast does the agent learn\n",
    "DISCOUNT = 0.95             # How important are future actions\n",
    "\n",
    "EPISODES = 10000            # Number of episodes\n",
    "\n",
    "epsilon = 0.5                           # Exploration rate\n",
    "START_EPSILON_DECAYING = 1              # First episode at which decay epsilon\n",
    "END_EPSILON_DECAYING = 1250             # Last episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846479feca214342a7ecf240bb41f7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=300000, description='Number Episodes', layout=Layout(width='40%'), max=300000, min=5000, step=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62a20461c824351bdae114010459f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.05, description='Learning rate', layout=Layout(width='40%'), max=1.0, step=0.01, style=Sli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bda58c717c41119a0f1e3c2fe9f6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.95, description='Discount factor', layout=Layout(width='40%'), max=1.0, step=0.01, style=S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a873d1e4b643349123a5d3d3ac8deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.9, description='Exploration rate', layout=Layout(width='40%'), max=1.0, step=0.05, style=S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flexible Parameters (ipywidgets)\n",
    "EPISODES_W = widgets.IntSlider(value=300000, min=5000, max=300000, step=50, description='Number Episodes', \n",
    "                               style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "LEARNING_RATE_W = widgets.FloatSlider(value=0.05, min=0, max=1, step=0.01, description='Learning rate',\n",
    "                                      style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "DISCOUNT_W = widgets.FloatSlider(value=0.95, min=0, max=1, step=0.01, description='Discount factor',\n",
    "                                 style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "epsilon_W = widgets.FloatSlider(value=0.9, min=0, max=1, step=0.05, description='Exploration rate',\n",
    "                                style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "display(EPISODES_W,LEARNING_RATE_W,DISCOUNT_W,epsilon_W)\n",
    "\n",
    "START_EPSILON_DECAYING = 1                      # First episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "\n",
    "discrete_partitions = 35\n",
    "\n",
    "# Retrieving \n",
    "EPISODES = EPISODES_W.value; LEARNING_RATE = LEARNING_RATE_W.value\n",
    "DISCOUNT = DISCOUNT_W.value; epsilon = epsilon_W.value\n",
    "epsilon_decaying_value = epsilon / ((EPISODES//1.5) - START_EPSILON_DECAYING)     # Amount of decayment of epsilon    \n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "DISCOUNT = 0.999\n",
    "\n",
    "\n",
    "# Generate the discrete state space and the interval of each discrete space\n",
    "discrete_state_space,discrete_state_intervals = sm.Q_Learning_Agent.generate_discrete_states(discrete_partitions,env)\n",
    "\n",
    "# Generate the q_table\n",
    "q_table = sm.Q_Learning_Agent.generate_q_table('random',env,discrete_state_space,low=-2,high=0)\n",
    "# q_table = sm.Q_Learning_Agent.generate_q_table('init_value',env,discrete_state_space,value=-2)\n",
    "\n",
    "# Rewards\n",
    "ep_rewards = list()\n",
    "success = list()\n",
    "epsilons = list()\n",
    "\n",
    "\n",
    "### Training\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)        # Discrete initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:                    # Randomize actions with epsilon\n",
    "            action = np.argmax(q_table[discrete_state])     # Action taken from the argmax of the current state\n",
    "        else:\n",
    "            action = env.action_space.sample()              # Action taken at random\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)       # Retrieve information\n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = sm.Q_Learning_Agent.get_discrete_state(new_state,env,discrete_state_intervals,discrete_partitions)  # Discretize new state\n",
    "        \n",
    "        if not done: \n",
    "            q_table = sm.Q_Learning_Agent.update_q_table(q_table,discrete_state,new_discrete_state,action,reward,LEARNING_RATE,DISCOUNT)\n",
    "        \n",
    "        elif terminated:\n",
    "            q_table[discrete_state + (action, )] = 0        # Update value when goal is reached\n",
    "        \n",
    "        discrete_state = new_discrete_state                 # Update state\n",
    "    \n",
    "    epsilon = sm.Q_Learning_Agent.decay_epsilon(epsilon,episode,epsilon_decaying_value,START_EPSILON_DECAYING,EPISODES//1.5)\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    success.append(terminated)\n",
    "    epsilons.append(epsilon)\n",
    "     \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'success' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Visualizing training results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sm\u001b[38;5;241m.\u001b[39mvisualizations\u001b[38;5;241m.\u001b[39mplot_moving_average([\u001b[43msuccess\u001b[49m],[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ-Learning\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                       title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance of Q-Learning Agent (Success)\u001b[39m\u001b[38;5;124m'\u001b[39m,ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccess rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m sm\u001b[38;5;241m.\u001b[39mvisualizations\u001b[38;5;241m.\u001b[39mplot_moving_average([ep_rewards],[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ-Learning\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                       title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance of Q-Learning Agent (Rewards)\u001b[39m\u001b[38;5;124m'\u001b[39m,ylabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRewards\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sm.visualizations.plot_series([epsilons],['epsilon'],title='Exploration Rate Decaying', ylabel='epsilon')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'success' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualizing training results\n",
    "sm.visualizations.plot_moving_average([success],['Q-Learning'],1000,\n",
    "                                      title='Performance of Q-Learning Agent (Success)',ylabel='Success rate')\n",
    "sm.visualizations.plot_moving_average([ep_rewards],['Q-Learning'],1000,\n",
    "                                      title='Performance of Q-Learning Agent (Rewards)',ylabel='Rewards')\n",
    "# sm.visualizations.plot_series([epsilons],['epsilon'],title='Exploration Rate Decaying', ylabel='epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Agent on 100 New Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of Q-Learning Agent on 100 episodes: 0.03\n",
      "Average rewards of Q-Learning Agent on 100 episodes: -497.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "testing_success = list()\n",
    "testing_rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "    testing_success.append(terminated)\n",
    "    testing_rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of Q-Learning Agent on 100 episodes: {round(sum(testing_success)/len(testing_success),2)}')\n",
    "print(f'Average rewards of Q-Learning Agent on 100 episodes: {round(sum(testing_rewards)/len(testing_rewards),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Advantage Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of (trained) A2C Agent on 100 episodes: 0.15\n",
      "Average rewards of (trained) A2C Agent on 100 episodes: -439.09\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "vec_env = make_vec_env(\"Acrobot-v1\", n_envs=4)\n",
    "\n",
    "A2C_model = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
    "A2C_model.learn(total_timesteps=100_000)\n",
    "\n",
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "testing_success = list()\n",
    "testing_rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _state = A2C_model.predict(state, deterministic=True)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "    testing_success.append(terminated)\n",
    "    testing_rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of (trained) A2C Agent on 100 episodes: {round(sum(testing_success)/len(testing_success),2)}')\n",
    "print(f'Average rewards of (trained) A2C Agent on 100 episodes: {round(sum(testing_rewards)/len(testing_rewards),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of (trained) DQN Agent on 100 episodes: 1.0\n",
      "Average rewards of (trained) DQN Agent on 100 episodes: -151.49\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "vec_env = make_vec_env(\"Acrobot-v1\", n_envs=4)\n",
    "\n",
    "DQN_model = DQN(\"MlpPolicy\", vec_env, verbose=0)\n",
    "DQN_model.learn(total_timesteps=100_000)\n",
    "\n",
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "testing_success = list()\n",
    "testing_rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _state = DQN_model.predict(state, deterministic=True)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "    testing_success.append(terminated)\n",
    "    testing_rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of (trained) DQN Agent on 100 episodes: {round(sum(testing_success)/len(testing_success),2)}')\n",
    "print(f'Average rewards of (trained) DQN Agent on 100 episodes: {round(sum(testing_rewards)/len(testing_rewards),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of (trained) PPO Agent on 100 episodes: 1.0\n",
      "Average rewards of (trained) PPO Agent on 100 episodes: -92.01\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(\"Acrobot-v1\", n_envs=4)\n",
    "\n",
    "PPO_model = PPO(\"MlpPolicy\", vec_env, verbose=0)\n",
    "PPO_model.learn(total_timesteps=100_000)\n",
    "\n",
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "testing_success = list()\n",
    "testing_rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _state = PPO_model.predict(state, deterministic=True)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "    testing_success.append(terminated)\n",
    "    testing_rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of (trained) PPO Agent on 100 episodes: {round(sum(testing_success)/len(testing_success),2)}')\n",
    "print(f'Average rewards of (trained) PPO Agent on 100 episodes: {round(sum(testing_rewards)/len(testing_rewards),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Trained Stable Baselines Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN_model\n",
    "\n",
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _state = model.predict(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
