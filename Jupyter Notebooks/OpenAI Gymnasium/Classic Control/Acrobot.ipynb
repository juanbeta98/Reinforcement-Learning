{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "import support_modules as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot\n",
    "\n",
    "## Description\n",
    "<div style=\"text-align: justify\">    \n",
    "The Acrobot environment is based on Sutton’s work in “Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding” and Sutton and Barto’s book. The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards.\n",
    "\n",
    "As seen in the Gif: two blue links connected by two green joints. The joint in between the two links is actuated. The goal is to swing the free end of the outer-link to reach the target height (black horizontal line above system) by applying torque on the actuator.\n",
    "</div>\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/acrobot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1',render_mode=None)\n",
    "\n",
    "rewards = list()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "    \n",
    "    rewards.append(ep_reward)\n",
    "\n",
    "env.close()\n",
    "print(f'Success rate: {sum(success)/len(success)}')\n",
    "print(f'Average reward: {sum(rewards)/len(rewards)}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Parameters\n",
    "LEARNING_RATE = 0.1         # How fast does the agent learn\n",
    "DISCOUNT = 0.95             # How important are future actions\n",
    "\n",
    "EPISODES = 10000            # Number of episodes\n",
    "\n",
    "epsilon = 0.5                           # Exploration rate\n",
    "START_EPSILON_DECAYING = 1              # First episode at which decay epsilon\n",
    "END_EPSILON_DECAYING = 1250             # Last episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3015d2539f764423bb4621094c8228c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2500, description='Episodes', max=10000, min=50, step=50)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0909d5fdb39456f8e3f520f3bec64c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, description='Learning r.', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912b312d2df949939b68a580715de7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.95, description='Discount f.', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b2f2e2f64640d095cab350fbefe799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Exploration r.', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flexible Parameters (ipywidgets)\n",
    "EPISODES_W = widgets.IntSlider(value=1000, min=50, max=5000, step=50, description='Number Episodes', \n",
    "                               style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "LEARNING_RATE_W = widgets.FloatSlider(value=0.1, min=0, max=1, step=0.05, description='Learning rate',\n",
    "                                      style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "DISCOUNT_W = widgets.FloatSlider(value=0.95, min=0, max=1, step=0.05, description='Discount factor',\n",
    "                                 style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "epsilon_W = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.05, description='Exploration rate',\n",
    "                                style={'handle_color':'magenta','description_width':'initial'}, layout={'width': '40%'})\n",
    "\n",
    "display(EPISODES_W,LEARNING_RATE_W,DISCOUNT_W,epsilon_W)\n",
    "\n",
    "START_EPSILON_DECAYING = 1                      # First episode at which decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "\n",
    "discrete_partitions = 40\n",
    "\n",
    "# Retrieving \n",
    "EPISODES = EPISODES_W.value; LEARNING_RATE = LEARNING_RATE_W.value\n",
    "DISCOUNT = DISCOUNT_W.value; epsilon = epsilon_W.value\n",
    "epsilon_decaying_value = epsilon / ((EPISODES//1.5) - START_EPSILON_DECAYING)     # Amount of decayment of epsilon    \n",
    "\n",
    "# Generate the discrete state space and the interval of each discrete space\n",
    "discrete_state_space,discrete_state_intervals = sm.Q_Learning_Agent.generate_discrete_states(discrete_partitions,env)\n",
    "\n",
    "# Generate the q_table\n",
    "q_table = sm.Q_Learning_Agent.generate_q_table('random',env,discrete_state_space,low=-2,high=0)\n",
    "\n",
    "# Rewards\n",
    "ep_rewards = list()\n",
    "success = list()\n",
    "epsilons = list()\n",
    "\n",
    "\n",
    "### Training\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)        # Discrete initial state\n",
    "    done = False\n",
    "    \n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:                    # Randomize actions with epsilon\n",
    "            action = np.argmax(q_table[discrete_state])     # Action taken from the argmax of the current state\n",
    "        else:\n",
    "            action = env.action_space.sample()              # Action taken at random\n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env.step(action)       # Retrieve information\n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        new_discrete_state = sm.Q_Learning_Agent.get_discrete_state(new_state,env,discrete_state_intervals,discrete_partitions)  # Discretize new state\n",
    "        \n",
    "        if not done: \n",
    "            q_table = sm.Q_Learning_Agent.update_q_table(q_table,discrete_state,new_discrete_state,action,reward,LEARNING_RATE,DISCOUNT)\n",
    "        \n",
    "        elif terminated:\n",
    "            q_table[discrete_state + (action, )] = 0        # Update value when goal is reached\n",
    "        \n",
    "        discrete_state = new_discrete_state                 # Update state\n",
    "    \n",
    "    epsilon = sm.Q_Learning_Agent.decay_epsilon(epsilon,episode,epsilon_decaying_value,START_EPSILON_DECAYING,EPISODES//1.5)\n",
    "    \n",
    "    ep_rewards.append(episode_reward)\n",
    "    success.append(terminated)\n",
    "    epsilons.append(epsilon)\n",
    "     \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing training results\n",
    "sm.visualizations.plot_moving_average([success],['Q-Learning'],50,\n",
    "                                      title='Performance of Q-Learning Agent (Success)',ylabel='Success rate')\n",
    "sm.visualizations.plot_moving_average([ep_rewards],['Q-Learning'],50,\n",
    "                                      title='Performance of Q-Learning Agent (Rewards)',ylabel='Rewards')\n",
    "# sm.visualizations.plot_series([epsilons],['epsilon'],title='Exploration Rate Decaying', ylabel='epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    env.render()\n",
    "    \n",
    "    done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Agent on 100 New Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode=None)\n",
    "success = list()\n",
    "rewards = list()\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        discrete_state = sm.Q_Learning_Agent.get_discrete_state(state,env,discrete_state_intervals,discrete_partitions)\n",
    "\n",
    "        ep_reward += reward\n",
    "        \n",
    "        done = sm.Q_Learning_Agent.evaluate_done(terminated,truncated)\n",
    "\n",
    "    success.append(terminated)\n",
    "    rewards.append(ep_reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Success rate of Q-Learning Agent on 100 episodes: {round(sum(success)/len(success),2)}')\n",
    "print(f'Average rewards of Q-Learning Agent on 100 episodes: {round(sum(rewards)/len(rewards),2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
