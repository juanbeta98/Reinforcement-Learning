{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a50ad0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Policy Evaluation\n",
    "\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "To understand the Policy Evaluation algorithm, some aditional concepts must be introduced. In first place, the policy $\\pi$ can be either *deterministic* or *stochastic*. For a *deterministic* policy, the decision given a state $s_t$ is certain. On contrary, for a *stochastic* policy, the decision $a$ given the state $s_t$ has an asociated probability expressed as $\\pi(a|s_t)$. In addition, the reward obtained from a transition given the state and decition might be as well *stochastic*. In this scenario, the transition probability function will have two arguments: $p[s_{t+1}, r|s_t, a]$ and the condition $\\sum_{s_{t+1}\\in{S_{t+1}},r \\in R_t)}{p[s_{t+1}, r|s_t,a]=1}\\space\\forall s_t\\in S_t, a_t\\in A_t$ must be satisfied (being $R_t$ the set of possible rewards on the step $t$). On this new terms, Bellman's equation for policy $\\pi$ is expressed as:\n",
    "</div> \n",
    "\n",
    "$$\n",
    "V_\\pi(s)=\\sum_{a\\in A_t}{\\pi(a|s_t)\\sum_{s_{t+1}\\in S_{t+1}, r \\space \\in R}{p[s_{t+1}, r| s_t, a][r+\\gamma V_\\pi(s_{t+1})]}}\n",
    "$$\n",
    "\n",
    "The expression states the recursive nature of the decision process. \n",
    "\n",
    "## Finding policy value through iterative process\n",
    "<br> <div style=\"text-align: justify\">\n",
    "The computation of the policy value might be aproximated by the **iterative policy evaluation** algorithm. In this algorithm, the value of $V(\\pi)$ for a given policy $\\pi$ will be aproximated by iterating over the policy, making an adjustment on every iteration until an accuracy threshold is acomplished. To perform this algorithm, the following steps must be performed. Firts, the accuracy threshold $\\theta$ must be aribtrarily determined. A arbitrary value for $V_0(s_t)$ will be assigned as a first aproximation to the policie's value for each state, not including all terminal states, which will have a value of 0. Thereafter, the policie's value will be updated in each iteration by the following rule:\n",
    "</div> \n",
    "\n",
    "$$\n",
    "V_{k+1}(s)=\\sum_{a\\in A_t}{\\pi(a|s_t)\\sum_{s_{t+1}, r}{p[s_{t+1}, r| s_t, a][r+\\gamma V_k(s_{t+1})]}}\n",
    "$$\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "for all non-terminal states. For each state that has it's value updated, the change $\\delta$ will be saved. At the end of the iteration of all states, the condition $\\delta > \\theta$ will be revised (where $\\delta$ is the maximum of all changes during the process). If the change has not achieved the desired accuracy, another iteration must be performed. this algorithm will continue untill the condition is not satisfied. A summary of the algorithm is shown bellow.\n",
    "</div> \n",
    "\n",
    "\n",
    "![](POLICY_EVAL.JPG)\n",
    "\n",
    "**Bibliography**\n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bab507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
