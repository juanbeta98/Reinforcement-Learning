##################### Policy Evaluation ##########################%% Problem 1: Sample Collection### Time steps of decisionT = [1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0]### SamplesSamples = [0, 250, 500, 750, 1000, 1250, "Succes"]### Space of StatesS_t = []for S in [0, 250, 500, 750]:    S_t.append((1000,S))for T in [900, 800, 700, 600, 500, 400, 300, 200, 100, 0]:    for S in Samples:        S_t.append((T,S))### Actions # CM for Calculated Maneuver# IM for improvised Maneuver A = ["CM", "IM"]### Transition probabilities## Transition Probabilites will be defined as dictionaries# Transition Probabilities for Calculated Maneuversp_CM = {}for s_t in S_t:    for s_tplus1 in S_t:        if s_tplus1[0] == s_t[0] - 100:            if s_t[1] != "Succes" and s_tplus1[1] != "Succes":                if s_tplus1[1] == s_t[1] + 250 and s_t[1] < 1250:                    p_CM[s_t, s_tplus1] = 0.3                elif s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:                    p_CM[s_t, s_tplus1] = 0.2                elif s_tplus1[1] == s_t[1] + 750 and s_t[1] < 750:                    p_CM[s_t, s_tplus1] = 0.1                elif s_tplus1[1] == s_t[1] and s_t[1] != "Succes":                    p_CM[s_t, s_tplus1] = 0.4                else:                     p_CM[s_t, s_tplus1] = 0            elif s_tplus1[1] == "Succes" and s_t[1] == 1000:                p_CM[s_t, s_tplus1] = 0.3            elif s_tplus1[1] == "Succes" and s_t[1] == 1250:                p_CM[s_t, s_tplus1] = 0.6            elif s_tplus1[1] == "Succes" and s_t[1] == 750:                p_CM[s_t, s_tplus1] = 0.1            elif s_tplus1[1] == s_t[1] and s_t[1] == "Succes":                p_CM[s_t, s_tplus1] = 1            else:                 p_CM[s_t, s_tplus1] = 0        else:            p_CM[s_t, s_tplus1] = 0    # Transition Probabilites for Improvised Maneuversp_IM = {}for s_t in S_t:    for s_tplus1 in S_t:        if s_tplus1[0] == s_t[0] - 100:            if s_t[1] != "Succes" and s_tplus1[1] != "Succes":                if s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:                    p_IM[s_t, s_tplus1] = 0.5                elif s_tplus1[1] == s_t[1] and s_t[1] <= 1250:                    p_IM[s_t, s_tplus1] = 0.5                else:                     p_IM[s_t, s_tplus1] = 0            elif s_tplus1[1] == "Succes" and (s_t[1] == 1000 or s_t[1] == 1250):                p_IM[s_t, s_tplus1] = 0.5            elif s_tplus1[1] == s_t[1] and s_t[1] == "Succes":                p_IM[s_t, s_tplus1] = 0.1            else:                 p_IM[s_t, s_tplus1] = 0        else:            p_IM[s_t, s_tplus1] = 0# Consolidation of probabilitiespTrans = {"CM":p_CM, "IM": p_IM}### Rewards# Reward r(s_t)r = {}for s_t in S_t:    r[s_t] = 0r[(0,"Succes")] = 1### Arbitrary Policy π## As the policy is stated as probabilities, this will also be defined as a dictionarypi = {}## Over all steps, if succes is achieved, then only Calculated Maneuvers will be performed# First 500 m of immersion: only calculated maneuvers will be performedfor s_t in S_t:    if s_t[0] in [1000, 900, 800, 700, 600]:        for a in A:            if a == "CM":                pi[s_t, a] = 1            else:                pi[s_t, a] = 0                # For stops in 500m, 400m and 300m: If s_t under 750, 60% for IM and 40% for CM.#                                   If s_t equal or over 750, 40% for IM and 60% for CMfor s_t in S_t:    if s_t[0] in [500, 400, 300]:        for a in A:            if s_t[1] == "Succes" and a == "CM":                pi[s_t, a] = 1            elif s_t[1] != "Succes":                if s_t[1] < 750 and a == "CM":                    pi[s_t, a] = 0.4                elif s_t[1] < 750 and a == "IM":                    pi[s_t, a] = 0.6                elif s_t[1] >= 750 and a == "CM":                    pi[s_t, a] = 0.6                elif s_t[1] >= 750 and a == "IM":                    pi[s_t, a] = 0.4            else:                pi[s_t, a] = 0# For last 200m of immersion: If s_t = 0 samples, a CM will be performed#                             If 250 <= s_t < 1000, 70% for IM and 30% for CM#                             If s_t >= 1000, 10% for IM and 70% for CMfor s_t in S_t:    if s_t[0] in [200, 100]:        for a in A:            if s_t[1] == "Succes" and a == "CM":                pi[s_t, a] = 1            elif s_t[1] != "Succes":                if s_t[1] == 0 and a == "CM":                    pi[s_t, a] = 1                elif s_t[1] < 1000 and a == "IM":                    pi[s_t, a] = 0.7                elif s_t[1] < 1000 and a == "CM":                    pi[s_t, a] = 0.3                elif s_t[1] >= 1000 and a == "IM":                    pi[s_t, a] = 0.1                elif s_t[1] >= 1000 and a == "CM":                    pi[s_t, a] = 0.9            else:                pi[s_t, a] = 0                ### Policy evaluation# Threshold theta indicates the desired accuracy of the finded valuetheta = 0.005# Defining dictionary V for the value of the policy for each state in 0.5 (arbitrarily)V = {}for s_t in S_t:    V[s_t] = 0.5for samples in [0, 250, 500, 750, 1000, 1250, "Succes"]:    V[(0,samples)] = 0# Delta being the change achieved, initailized in 0delta = 10# Iterationswhile delta > theta:    delta = 0    for s_t in S_t:        if s_t[0] != 0:            v = V[s_t]            value = 0            for a in A:                value1 = 0                for s_tplus1 in S_t:                    value1 += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1])                value += pi[s_t,a] * value1               V[s_t] = value            delta = max(delta, abs(v - V[s_t]))print(f"The probability of succes at the beggining of the immersion is {round(V[(1000,0)],3)}")print(f"The probability of succes at 500m of immersion and 250 samples  is {round(V[(500,250)],3)}")print(f"The probability of succes at 700m of immersion and already 1500 samples is {round(V[(700,'Succes')],3)}")#%% Problem 1: Sample Collection (Optimal policy evaluated with IPE)### Time steps of decisionT = [1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0]### SamplesSamples = [0, 250, 500, 750, 1000, 1250, "Succes"]### Space of StatesS_t = []for S in [0, 250, 500, 750]:    S_t.append((1000,S))for T in [900, 800, 700, 600, 500, 400, 300, 200, 100, 0]:    for S in Samples:        S_t.append((T,S))### Actions # CM for Calculated Maneuver# IM for improvised Maneuver A = ["CM", "IM"]### Transition probabilities## Transition Probabilites will be defined as dictionaries# Transition Probabilities for Calculated Maneuversp_CM = {}for s_t in S_t:    for s_tplus1 in S_t:        if s_tplus1[0] == s_t[0] - 100:            if s_t[1] != "Succes" and s_tplus1[1] != "Succes":                if s_tplus1[1] == s_t[1] + 250 and s_t[1] < 1250:                    p_CM[s_t, s_tplus1] = 0.3                elif s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:                    p_CM[s_t, s_tplus1] = 0.2                elif s_tplus1[1] == s_t[1] + 750 and s_t[1] < 750:                    p_CM[s_t, s_tplus1] = 0.1                elif s_tplus1[1] == s_t[1] and s_t[1] != "Succes":                    p_CM[s_t, s_tplus1] = 0.4                else:                     p_CM[s_t, s_tplus1] = 0            elif s_tplus1[1] == "Succes" and s_t[1] == 1000:                p_CM[s_t, s_tplus1] = 0.3            elif s_tplus1[1] == "Succes" and s_t[1] == 1250:                p_CM[s_t, s_tplus1] = 0.6            elif s_tplus1[1] == "Succes" and s_t[1] == 750:                p_CM[s_t, s_tplus1] = 0.1            elif s_tplus1[1] == s_t[1] and s_t[1] == "Succes":                p_CM[s_t, s_tplus1] = 1            else:                 p_CM[s_t, s_tplus1] = 0        else:            p_CM[s_t, s_tplus1] = 0    # Transition Probabilites for Improvised Maneuversp_IM = {}for s_t in S_t:    for s_tplus1 in S_t:        if s_tplus1[0] == s_t[0] - 100:            if s_t[1] != "Succes" and s_tplus1[1] != "Succes":                if s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:                    p_IM[s_t, s_tplus1] = 0.5                elif s_tplus1[1] == s_t[1] and s_t[1] <= 1250:                    p_IM[s_t, s_tplus1] = 0.5                else:                     p_IM[s_t, s_tplus1] = 0            elif s_tplus1[1] == "Succes" and (s_t[1] == 1000 or s_t[1] == 1250):                p_IM[s_t, s_tplus1] = 0.5            elif s_tplus1[1] == s_t[1] and s_t[1] == "Succes":                p_IM[s_t, s_tplus1] = 0.1            else:                 p_IM[s_t, s_tplus1] = 0        else:            p_IM[s_t, s_tplus1] = 0# Consolidation of probabilitiespTrans = {"CM":p_CM, "IM": p_IM}### Rewards# Reward r(s_t)r = {}for s_t in S_t:    r[s_t] = 0r[(0,"Succes")] = 1### Arbitrary Policy π## As the policy is stated as probabilities this will also be defined as a dictionarypi = {}for s_t in S_t:    if s_t[0] in [1000, 900]:        for a in A:            if a == "CM":                pi[s_t, a] = 1            else:                pi[s_t, a] = 0                for s_t in S_t:    if s_t[0] in [800]:        for a in A:            if a == "CM" and s_t[1] != 0:                pi[s_t, a] = 1            elif a == "IM" and s_t[1] == 0:                pi[s_t, a] = 1            else:                 pi[s_t,a] = 0for s_t in S_t:    if s_t[0] in [700,600]:        for a in A:            if a == "IM" and (s_t[1] == 0 or s_t[1] == 500):                pi[s_t, a] = 1            elif a == "CM" and s_t[1] != 0 and s_t[1] != 500:                pi[s_t, a] = 1            else:                 pi[s_t,a] = 0        for s_t in S_t:    if s_t[0] in [500,400,300]:        for a in A:            if a == "IM" and (s_t[1] == 0 or s_t[1] == 500 or s_t[1] == 1000):                pi[s_t, a] = 1            elif a == "CM" and s_t[1] != 0 and s_t[1] != 500 and s_t[1] != 1000:                pi[s_t, a] = 1            else:                 pi[s_t,a] = 0 for s_t in S_t:    if s_t[0] in [200]:        for a in A:            if a == "IM" and (s_t[1] == 500 or s_t[1] == 1000):                pi[s_t, a] = 1            elif a == "CM" and s_t[1] != 500 and s_t[1] != 1000:                pi[s_t, a] = 1            else:                 pi[s_t,a] = 0for s_t in S_t:    if s_t[0] in [100]:        for a in A:            if a == "IM" and s_t[1] == 1000:                pi[s_t, a] = 1            elif a == "CM" and s_t[1] != 1000:                pi[s_t, a] = 1            else:                 pi[s_t,a] = 0### Policy evaluation# Threshold theta indicates the desired accuracy of the finded valuetheta = 0.005# Defining dictionary V for the value of the policy for each state in 0.5 (arbitrarily)V = {}for s_t in S_t:    V[s_t] = 0.5for samples in [0, 250, 500, 750, 1000, 1250]:    V[(0,samples)] = 0V[0,"Succes"] = 0# Delta being the change achieved, initailized in 0delta = 10# Iterationswhile delta > theta:    delta = 0    for s_t in S_t:        if s_t[0] != 0:            v = V[s_t]            value = 0            for a in A:                value1 = 0                for s_tplus1 in S_t:                    value1 += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1])                value += pi[s_t,a] * value1               V[s_t] = value            delta = max(delta, abs(v - V[s_t]))#%% Problem 2: Aqueduct### StatesS = list(range(3,11))### DecisionsA = ["Send", "Don't Send"]### Transition probabilities# Send the cleaning teamp_SEND = {}for s_t in S:    for s_tplus1 in S:        if s_tplus1 == 3:            p_SEND[s_t, s_tplus1] = 1        else:            p_SEND[s_t, s_tplus1] = 0# Dont's send the cleaning team p_DONTS = {}for s_t in S:    for s_tplus1 in S:        if s_tplus1 >= s_t:            p_DONTS[s_t, s_tplus1] = 1/(11-s_t)        else:            p_DONTS[s_t, s_tplus1] = 0# Consolidation of probabilitiespTrans = {"Send": p_SEND, "Don't Send": p_DONTS}### Costsc = {}for s_t in S:    c[s_t, "Send"] = 12 + s_t * 0.01 * 200    c[s_t, "Don't Send"] =  s_t * 0.01 * 200### Discount Factor gamma = 0.95### Arbitrary policy πpi = {}for s_t in S:    if s_t == 10:        pi[s_t, "Send"] = 1        pi[s_t, "Don't Send"] = 0    else:         pi[s_t, "Send"] = 0        pi[s_t, "Don't Send"] = 1        ### Policy Evaluation# Threshold theta indicates the desired accuracy of the finded valuetheta = 0.005# Defining dictionary V for the value of the policy for each state in 0.5 (arbitrarily)V = {}for s_t in S:    V[s_t] = 300# Delta being the change achieved, initailized in 0delta = 10# Iterationswhile delta > theta:    delta = 0    for s_t in S:            v = V[s_t]            value = 0            for a in A:                value1 = 0                for s_tplus1 in S:                    value1 += pTrans[a][s_t, s_tplus1] * (c[s_t,a] + gamma * V[s_tplus1])                value += pi[s_t,a] * value1               V[s_t] = value            delta = max(delta, abs(v - V[s_t]))print(f"The expected cost when there are 4 liters of trash is ${round(V[4], 3)} million pesos")print(f"The expected cost when there are 7 liters of trash is ${round(V[7], 3)} million pesos")print(f"The expected cost when there are 10 liters of trash is ${round(V[10], 3)} million pesos")