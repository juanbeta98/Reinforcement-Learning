{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a635a9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Policy Iteration\n",
    "\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "Once a *deterministc* policy is evaluated (through Iterative Policy Iteration), it might be improved by changing the decision in one single state, if this improves the policy's value for that state. With this change, a new (but quite similiar) policy is created, so, this new policy must be evaluated. This process repited on a loop is the escence of Policy Iteration. At the end, no decision change of any state will improve the policy, therefore, the optimal policy has been found. \n",
    "</div>\n",
    "\n",
    "**Policy Improvement**\n",
    "<br><div style=\"text-align: justify\">    \n",
    "Iterative Policy Evaluation has already been reviewed, therfore, there will not be a detailed explanation of this algorithm. Once an arbitrary policy's value has been computed, deriving new policies is a simple task through **Policy Improvement**. Considering $\\pi(s_t)$ is the decision taken in the state $s_t$ with policy $\\pi$, it is in interest to compare this states policy value to that achieved with another decision $a$ &#8800; $\\pi(s_t)$. The new policy's value can be found with the following expresion, where $q_{\\pi}(s_t,a)$ is the value of the state $s_t$ when taking decision $a$ and following the existing policy $\\pi$ thereafter:\n",
    "</div>\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s_t,a)=\\sum_{s_{t+1},r}{p(s_{t+1},r|s_t,a)\\left[r+\\gamma V_\\pi(s_{t+1}) \\right]}\n",
    "$$\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "Back to policy iteration, the process of evaluating a policy to find its value, improving it and evaluating the new policy will be repeted over and over until there is no improvement. Then, the optimal policy and the optimal value has been found. This is shown in the following diagram where $E$ means *Policy Evaluation* and $I$ means *Policy Improvement*:\n",
    "</div>\n",
    "\n",
    "<br> <div style=\"text-align: center\"> \n",
    "$\\pi_0$ &#8594; $E$ &#8594; $v_{\\pi_0}$ &#8594; $I$ &#8594; $\\pi_1$ &#8594; $E$ &#8594; $v_{\\pi_1}$ &#8594; $I$ &#8594; $\\pi_2$ &#8594; $E$ &#8594; $v_{\\pi_2}$ &#8594; $I$ &#8594; $\\pi_3$ &#8594; ... &#8594; $I$ &#8594; $\\pi_*$ &#8594; $E$ &#8594; $v_{*}$\n",
    "\n",
    "A sumary of the algorithm is presented below:\n",
    "    \n",
    "![](POLICY_ITER.JPG)\n",
    "    \n",
    "**Bibliography**\n",
    "\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
