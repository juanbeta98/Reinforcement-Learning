{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Sample Recolection\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The first step to applying the Policy Evaluation Algorithm to this problem will be coding all of the initial conditions of the problem. We start by coding the time steps, the possible number of samples and the possible actions to be taken. Due to the fact that this is a finite, *non-stationary* MDP, combinations of time steps and number of samples will be considered the states of the problem. For example, 1250 samples at 500m of immersion will be an state and 1250 samples at 400m of immersion will be another state. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time steps of decision\n",
    "T = [1000, 900, 800, 700, 600, 500, 400, 300, 200, 100, 0]\n",
    "\n",
    "### Samples\n",
    "Samples = [0, 250, 500, 750, 1000, 1250, \"Succes\"]\n",
    "\n",
    "### Space of States\n",
    "S_t = []\n",
    "for S in [0, 250, 500, 750]:\n",
    "    S_t.append((1000,S))\n",
    "for T in [900, 800, 700, 600, 500, 400, 300, 200, 100, 0]:\n",
    "    for S in Samples:\n",
    "        S_t.append((T,S))\n",
    "\n",
    "### Actions \n",
    "# CM for Calculated Maneuver\n",
    "# IM for improvised Maneuver \n",
    "A = [\"CM\", \"IM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Next, transition probabilities will be coded. As mention before, this probabilities will indicate the stochastic nature of the problem on the transitions between states. As the states contain steps of decision and number of samples, this information will be taken into account in the transition probabilities. Also, the probabilities depend on the action that is performed. Therefore, there will be to dictionaries of probabilites, one for Calculated Maneuvers and one for Improvised Maneuvers. At the end, both of this dictionaries will be consolidated in one dictionary. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transition probabilities\n",
    "## Transition Probabilites will be defined as dictionaries\n",
    "# Transition Probabilities for Calculated Maneuvers\n",
    "p_CM = {}\n",
    "for s_t in S_t:\n",
    "    for s_tplus1 in S_t:\n",
    "        if s_tplus1[0] == s_t[0] - 100:\n",
    "            if s_t[1] != \"Succes\" and s_tplus1[1] != \"Succes\":\n",
    "                if s_tplus1[1] == s_t[1] + 250 and s_t[1] < 1250:\n",
    "                    p_CM[s_t, s_tplus1] = 0.3\n",
    "                elif s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:\n",
    "                    p_CM[s_t, s_tplus1] = 0.2\n",
    "                elif s_tplus1[1] == s_t[1] + 750 and s_t[1] < 750:\n",
    "                    p_CM[s_t, s_tplus1] = 0.1\n",
    "                elif s_tplus1[1] == s_t[1] and s_t[1] != \"Succes\":\n",
    "                    p_CM[s_t, s_tplus1] = 0.4\n",
    "                else: \n",
    "                    p_CM[s_t, s_tplus1] = 0\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 1000:\n",
    "                p_CM[s_t, s_tplus1] = 0.3\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 1250:\n",
    "                p_CM[s_t, s_tplus1] = 0.6\n",
    "            elif s_tplus1[1] == \"Succes\" and s_t[1] == 750:\n",
    "                p_CM[s_t, s_tplus1] = 0.1\n",
    "            elif s_tplus1[1] == s_t[1] and s_t[1] == \"Succes\":\n",
    "                p_CM[s_t, s_tplus1] = 1\n",
    "            else: \n",
    "                p_CM[s_t, s_tplus1] = 0\n",
    "        else:\n",
    "            p_CM[s_t, s_tplus1] = 0\n",
    "    \n",
    "# Transition Probabilites for Improvised Maneuvers\n",
    "p_IM = {}\n",
    "for s_t in S_t:\n",
    "    for s_tplus1 in S_t:\n",
    "        if s_tplus1[0] == s_t[0] - 100:\n",
    "            if s_t[1] != \"Succes\" and s_tplus1[1] != \"Succes\":\n",
    "                if s_tplus1[1] == s_t[1] + 500 and s_t[1] < 1000:\n",
    "                    p_IM[s_t, s_tplus1] = 0.5\n",
    "                elif s_tplus1[1] == s_t[1] and s_t[1] <= 1250:\n",
    "                    p_IM[s_t, s_tplus1] = 0.5\n",
    "                else: \n",
    "                    p_IM[s_t, s_tplus1] = 0\n",
    "            elif s_tplus1[1] == \"Succes\" and (s_t[1] == 1000 or s_t[1] == 1250):\n",
    "                p_IM[s_t, s_tplus1] = 0.5\n",
    "            elif s_tplus1[1] == s_t[1] and s_t[1] == \"Succes\":\n",
    "                p_IM[s_t, s_tplus1] = 0.1\n",
    "            else: \n",
    "                p_IM[s_t, s_tplus1] = 0\n",
    "        else:\n",
    "            p_IM[s_t, s_tplus1] = 0\n",
    "\n",
    "# Consolidation of probabilities\n",
    "pTrans = {\"CM\":p_CM, \"IM\": p_IM}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <div style=\"text-align: justify\"> \n",
    "Finally, to finish the basic definition of the problem, the rewards must be defined. As mentioned in the problem's definition, the only reward will be if the mission has succeded at the end of the immersion. Meaning, a reward of 1 will be recieved only at state (\"Succes\", 0).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rewards\n",
    "# Reward r(s_t)\n",
    "r = {}\n",
    "for s_t in S_t:\n",
    "    r[s_t] = 0\n",
    "\n",
    "r[(0,\"Succes\")] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy $\\pi$ to evaluate**\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "To apply Policy Evaluation, an arbitrary policy is requied. For implementation, an arbitrary policy will be defined as one is not provided by the exercise. For this application, the policy will be a *stochastic* policy described next. For the first 500 meters of immersion (1000m, 900m, 800m, 700m, 600m), only calculated maneuvers can be performed as these are safer for the ship and its tripulation. For the following 3 stops of the immersion (500m, 400m, 300m), if the samples are under 750, there will be a 60% chance of applying an improvised maneuver and 40% chance of applying a calculated maneuver. If the samples collected are 750 or more, the probabilities will be reversed. Finally, for the last 2 stops of the immersion (200m, 100m), if there are 0 samples collected, a calculated maneuver will be performed. If the samples are between 250 and 1000, an improvised maneuver will be performed with a probability of 0,7 and a calculated maneuver with 0,3. Finally, if there are over 1000 samples, a calculated maneuver will be performed with a probability of 0,9 and an improvised with 0,1. Always the mission has achieved 1500 samples (**succes**), a calculated maneuver will be performed in all next stops. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbitrary Policy Ï€\n",
    "## As the policy is stated as probabilities, this will also be defined as a dictionary\n",
    "pi = {}\n",
    "\n",
    "## Over all steps, if succes is achieved, then only Calculated Maneuvers will be performed\n",
    "# First 500 m of immersion: only calculated maneuvers will be performed\n",
    "for s_t in S_t:\n",
    "    if s_t[0] in [1000, 900, 800, 700, 600]:\n",
    "        for a in A:\n",
    "            if a == \"CM\":\n",
    "                pi[s_t, a] = 1\n",
    "            else:\n",
    "                pi[s_t, a] = 0\n",
    "                \n",
    "# For stops in 500m, 400m and 300m: If s_t under 750, 60% for IM and 40% for CM.\n",
    "#                                   If s_t equal or over 750, 40% for IM and 60% for CM\n",
    "for s_t in S_t:\n",
    "    if s_t[0] in [500, 400, 300]:\n",
    "        for a in A:\n",
    "            if s_t[1] == \"Succes\" and a == \"CM\":\n",
    "                pi[s_t, a] = 1\n",
    "            elif s_t[1] != \"Succes\":\n",
    "                if s_t[1] < 750 and a == \"CM\":\n",
    "                    pi[s_t, a] = 0.4\n",
    "                elif s_t[1] < 750 and a == \"IM\":\n",
    "                    pi[s_t, a] = 0.6\n",
    "                elif s_t[1] >= 750 and a == \"CM\":\n",
    "                    pi[s_t, a] = 0.6\n",
    "                elif s_t[1] >= 750 and a == \"IM\":\n",
    "                    pi[s_t, a] = 0.4\n",
    "            else:\n",
    "                pi[s_t, a] = 0\n",
    "\n",
    "# For last 200m of immersion: If s_t = 0 samples, a CM will be performed\n",
    "#                             If 250 <= s_t < 1000, 70% for IM and 30% for CM\n",
    "#                             If s_t >= 1000, 10% for IM and 70% for CM\n",
    "for s_t in S_t:\n",
    "    if s_t[0] in [200, 100]:\n",
    "        for a in A:\n",
    "            if s_t[1] == \"Succes\" and a == \"CM\":\n",
    "                pi[s_t, a] = 1\n",
    "            elif s_t[1] != \"Succes\":\n",
    "                if s_t[1] == 0 and a == \"CM\":\n",
    "                    pi[s_t, a] = 1\n",
    "                elif s_t[1] < 1000 and a == \"IM\":\n",
    "                    pi[s_t, a] = 0.7\n",
    "                elif s_t[1] < 1000 and a == \"CM\":\n",
    "                    pi[s_t, a] = 0.3\n",
    "                elif s_t[1] >= 1000 and a == \"IM\":\n",
    "                    pi[s_t, a] = 0.1\n",
    "                elif s_t[1] >= 1000 and a == \"CM\":\n",
    "                    pi[s_t, a] = 0.9\n",
    "            else:\n",
    "                pi[s_t, a] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "Since policy $\\pi$ is defined as a probability for each step, on each state for a decision, evaluation Policy can be implemented. For this process, the threshold $\\theta$ will be 0.005. Then, a dictionary that will store the policy values for each state is defined as $V(s_t)$. All the values are arbitrarily set to 0.5 for the non-terminal states, for the terminal states (when the exploration has ended at 0m) this value will be 0. After this, the value of $\\delta$ is initialized as 10 so the iterative process in the loop can start. As soon as the iterative process starts, the value of $\\delta$ is set to 0. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Policy evaluation\n",
    "# Threshold theta indicates the desired accuracy of the finded value\n",
    "theta = 0.005\n",
    "\n",
    "# Defining dictionary V for the value of the policy for each state in 0.5 (arbitrarily)\n",
    "V = {}\n",
    "for s_t in S_t:\n",
    "    V[s_t] = 0.5\n",
    "for samples in [0, 250, 500, 750, 1000, 1250, \"Succes\"]:\n",
    "    V[(0,samples)] = 0\n",
    "\n",
    "# Delta being the change achieved, initailized in 0\n",
    "delta = 10\n",
    "\n",
    "# Iterations\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s_t in S_t:\n",
    "        if s_t[0] != 0:\n",
    "            v = V[s_t]\n",
    "            value = 0\n",
    "            for a in A:\n",
    "                value1 = 0\n",
    "                for s_tplus1 in S_t:\n",
    "                    value1 += pTrans[a][s_t, s_tplus1] * (r[s_tplus1] + V[s_tplus1])\n",
    "                value += pi[s_t,a] * value1   \n",
    "            V[s_t] = value\n",
    "            delta = max(delta, abs(v - V[s_t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "At this point, the Policy Evaluation is completed. In the way this problem has been constructed, the policy's values indicate the probability of succes of the mission in a given state. For example, the overall succes can be finded by looking at the policy's value for the state (1000 meters, 0 samples), when the immersion has just started and none samples have been gathered. In addition, an specific scenario can also be analized. For example, the scenario where at the middle of the immersion, only 250 samples have been gathered. Of course, all of the states where succes has already been achieved will have a probability of succes of 1. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of succes at the beggining of the immersion is 0.926\n",
      "The probability of succes at 500m of immersion and 250 samples  is 0.564\n",
      "The probability of succes at 700m of immersion and already 1500 samples is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The probability of succes at the beggining of the immersion is {round(V[(1000,0)],3)}\")\n",
    "print(f\"The probability of succes at 500m of immersion and 250 samples  is {round(V[(500,250)],3)}\")\n",
    "print(f\"The probability of succes at 700m of immersion and already 1500 samples is {round(V[(700,'Succes')],3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Aqueduct\n",
    "<br> <div style=\"text-align: justify\"> \n",
    "The same iterative process will be performed in this problem with some distinctions due to the characteristics of the problem. First, this is a *infinite* MDP, meaning the time steps have no valuable information. Considering this, states will contain only the liters of trash. Second, a discount factor will be applyied as stated in the description of the algorithm. On this terms, the basics of the problem will be coded in the next section: States, Transition Probabilities, Costs and the Discount Factor\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### States\n",
    "S = list(range(3,11))\n",
    "\n",
    "### Decisions\n",
    "A = [\"Send\", \"Don't Send\"]\n",
    "\n",
    "### Transition probabilities\n",
    "# Send the cleaning team\n",
    "p_SEND = {}\n",
    "for s_t in S:\n",
    "    for s_tplus1 in S:\n",
    "        if s_tplus1 == 3:\n",
    "            p_SEND[s_t, s_tplus1] = 1\n",
    "        else:\n",
    "            p_SEND[s_t, s_tplus1] = 0\n",
    "\n",
    "# Dont's send the cleaning team \n",
    "p_DONTS = {}\n",
    "for s_t in S:\n",
    "    for s_tplus1 in S:\n",
    "        if s_tplus1 >= s_t:\n",
    "            p_DONTS[s_t, s_tplus1] = 1/(11-s_t)\n",
    "        else:\n",
    "            p_DONTS[s_t, s_tplus1] = 0\n",
    "\n",
    "# Consolidation of probabilities\n",
    "pTrans = {\"Send\": p_SEND, \"Don't Send\": p_DONTS}\n",
    "\n",
    "### Costs\n",
    "c = {}\n",
    "for s_t in S:\n",
    "    c[s_t, \"Send\"] = 12 + s_t * 0.01 * 200\n",
    "    c[s_t, \"Don't Send\"] =  s_t * 0.01 * 200\n",
    "\n",
    "### Discount Factor \n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy $\\pi$ to evaluate**\n",
    "<div style=\"text-align: justify\"> \n",
    "As stated in the instructions, the policy we're interesed on evaluating is sending the cleaning team ONLY when the trash has reached the maximum number of liters. This is, 10 liters. In any other scenario, the team will not be sent. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arbitrary policy Ï€\n",
    "pi = {}\n",
    "for s_t in S:\n",
    "    if s_t == 10:\n",
    "        pi[s_t, \"Send\"] = 1\n",
    "        pi[s_t, \"Don't Send\"] = 0\n",
    "    else: \n",
    "        pi[s_t, \"Send\"] = 0\n",
    "        pi[s_t, \"Don't Send\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "With all components of the MDP defined, the iterative policy evaluation can be performed. The $\\theta$ threshold will be the same as the one used in the previous problem. The arbitrary policy's initial value for all states will be $300 million pesos. There are no terminal states in this problem. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Policy Evaluation\n",
    "# Threshold theta indicates the desired accuracy of the finded value\n",
    "theta = 0.005\n",
    "\n",
    "# Defining dictionary V for the value of the policy for each state in 0.5 (arbitrarily)\n",
    "V = {}\n",
    "for s_t in S:\n",
    "    V[s_t] = 300\n",
    "\n",
    "# Delta being the change achieved, initailized in 0\n",
    "delta = 10\n",
    "\n",
    "# Iterations\n",
    "while delta > theta:\n",
    "    delta = 0\n",
    "    for s_t in S:\n",
    "            v = V[s_t]\n",
    "            value = 0\n",
    "            for a in A:\n",
    "                value1 = 0\n",
    "                for s_tplus1 in S:\n",
    "                    value1 += pTrans[a][s_t, s_tplus1] * (c[s_t,a] + gamma * V[s_tplus1])\n",
    "                value += pi[s_t,a] * value1   \n",
    "            V[s_t] = value\n",
    "            delta = max(delta, abs(v - V[s_t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iterative Policy Evaluation is done. With the values found, it is possible to determine the expected cost associated with any of the states when following this policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected cost when there are 4 liters of trash is $321.864 million pesos\n",
      "The expected cost when there are 7 liters of trash is $331.6 million pesos\n",
      "The expected cost when there are 10 liters of trash is $334.518 million pesos\n"
     ]
    }
   ],
   "source": [
    "print(f\"The expected cost when there are 4 liters of trash is ${round(V[4], 3)} million pesos\")\n",
    "print(f\"The expected cost when there are 7 liters of trash is ${round(V[7], 3)} million pesos\")\n",
    "print(f\"The expected cost when there are 10 liters of trash is ${round(V[10], 3)} million pesos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
